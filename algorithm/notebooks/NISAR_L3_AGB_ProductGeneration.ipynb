{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ed4451-b706-4c67-88d0-635d35fc35a0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../../../nisar_leno_biomass_files/NISAR_Mission_Logo.png\" alt=\"key.png\">  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614a824-39d2-4a9e-a4ad-ba28ab63b5a9",
   "metadata": {},
   "source": [
    "# NASA ISRO Synthetic Aperture Radar Mission\n",
    "<br>\n",
    "\n",
    "## Algorithm Theoretical Basis Document and Jupyter Notebook for NISAR Biomass Retrieval\n",
    "<br>\n",
    "\n",
    "### Authors: \n",
    "> Naveen Ramachandran, KC Cushman, Alexandra Christensen, & Sassan Saatchi <br>\n",
    "> Carbon Cycle and Ecosystems <br>\n",
    "> Jet Propulsion Laboratory <br>\n",
    "> California Institute of Technology <br>\n",
    "> Date: 2025-07-18 <br>\n",
    "\n",
    "### Summary\n",
    "> This notebook describes the algorithm theoretical basis document (ATBD) for retrieving forest aboveground biomass (AGB) from a NISAR simulated time-series data stacks. The algorithm is designed to meet the Level 3 Science requirements for generating AGB product annually. This notebook constitutes a combination of formulating the theoretical basis for the NISAR Forest Biomass algorithm that is based on semi-empirical model and an implementation of the algorithm in executable python code. \n",
    "\n",
    "> A test data set accompanies the notebook that is available from NASA's Alaska Satellite Facility (ASF) and EarthExplorer and referenced in this notebook. Also accomanying the notebook is a python code library module (`atbd_biomass.py`) that is imported into this notebook in order for the example of the algorithm to execute within the notebook.\n",
    "\n",
    "> In this notebook, we will walk through the various steps involved for estimation of NISAR L3 AGB product from NISAR simulated UAVSAR data. To illustrate the usage of NISAR AGB Model, we will use a NISAR simulated ALOS2 datasets acquired during NISAR Calibration/Validation phase.\n",
    "\n",
    "> This ATBD notebook consists of following tasks:- \n",
    "> 1. **Data Downloading and Analysis** : This step performs data downloading and estimation of image statistics in HH and HV Polarization.\n",
    "> 2. **Create Forest/Non-Forest Mask** : This steo generates FNF mask the SAR scene.\n",
    "> 3. **Loading Calibrated Model & initial AGB Estimation** : This step loads the calibration parameters of NISAR biomass algorithm and estimates initial AGB values. \n",
    "> 4. **L3 Biomass Product Generation** : This str[ produces L3 Biomass Map over the entire sites.\n",
    "\n",
    "> **The NISAR requirement for BIOMASS product is** *to measure above ground woody vegetation biomass annually at the hectare scale (1 ha) to an RMS accuracy of 20 Mg/ha for 80% of areas of biomass less than 100 Mg/ha.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8ea34-ffaa-4b97-9744-23ad7d74e1ae",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>\n",
    "## Table of Contents\n",
    "1. [Context on Forest Biomass and NISAR Time series](#section-1)\n",
    "2. [Canopy Model for Radar Backscattering](#SEC_2) <br>\n",
    "    2.1. [Basic Assumptions](#SEC_21) <br>\n",
    "    2.2. [Further Simplifications](#SEC_22) <br>\n",
    "    2.3. [Final Parameters for Retrieval Algorithm](#SEC_23) <br>\n",
    "3. [Implementation of NISAR BIOMASS Algorithm](#SEC_3)\n",
    "4. [Example of NISAR BIOMASS Algorithm Calibration/Validation](#SEC_4) <br>\n",
    "    4.1. [Execute Notebook 0: Data Downloading and Analysis](#SEC_41) <br>\n",
    "    4.2. [Execute Notebook 1: Create Calibration / Validation Products](#SEC_42) <br>\n",
    "    4.3. [Execute Notebook 2: NISAR Model Calibration & Validation](#SEC_43) <br>\n",
    "    4.4. [Execute Notebook 3: L3 Biomass Product Generation](#SEC_44) <br>\n",
    "    4.5. [Execute Notebook 4: Uploading L3 Biomass products to Cal./Val and/or S3 Database](#SEC_45) <br>\n",
    "5. [References](#SEC_5) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5413a-6583-40a3-8a5d-d4dce74a9599",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "## 1. Context on Forest Biomass and NISAR Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeee2d-efd2-49dd-866c-55157e5b6b0a",
   "metadata": {},
   "source": [
    "> Forest aboveground biomass (AGB) is a underlying parameter for predicting the spatial distribution of carbon in the terrestrial ecosystem. Also, it is vital from an economic (assits food, timber, and energy industry), disaster (forest fire, etc.), soil (soil errosion, landslide, etc.), water (flooding, water quality, etc.) biodoversity management (Foley et al. 2005; Chazdon 2008). However, the current knowledge of the distribution and magnitude of terrestrial biomass is based almost entirely on ground measurements over an extremely small, and possibly biased sample, with almost no measurements in the southern hemisphere and equatorial regions (Schimel et al., 2015). Forest AGB is directly related to the structural attributes of the forest. Synthetic Aperture Radar (SAR) backscatter measurements are sensitive to forest strctural information. Hence it can be correlated to AGB. However, the SAR backscatter sensitivity to AGB varies depending on the wavelength and geometry of the radar measurements, and is influenced by the surface topography, structure of vegetation, and environmental conditions such as soil moisture and vegetation phenology or moisture.\n",
    "\n",
    "> For the NISAR mission, the focus was on AGB values between 0- 100 Mg/ha, as the sensitivity of L-band backscatter measurements to AGB saturates around 100 Mg/ha (Yu Lin 2016), which covers 50% of the global forests and the entire area of other woody vegetation (FRA, 2010). Also, with wide swath (240 km), high resolution (~15m), 12-day repeat orbit cycle, it allow retrieval of AGB at high spatial resolution over global scale temporally, allowing us to understand the dynamics of forest ecosystem. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bdf31-badf-461d-8028-0f51218246e8",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2\"></a>\n",
    "## 2. Canopy Model for Radar Backscattering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610b664-938a-4f18-8cda-f60bd59fdb4b",
   "metadata": {},
   "source": [
    "\n",
    ">The physical based data-fitted model (Saatchi and McDonald, 1997) describes scattering mechanisms of the forest with three  components **(Fig. 1)**, namely direct **D** , Interactive **I** , i.e., direct-reflect, and ground **G** components.  \n",
    "\n",
    ">The **D** component consists of the scattering from trunks, branches, and leaves directly back to the sensor.  The **I** terms, also known as \"double bounce\" term, is the scattering that includes interaction between trees and ground.  This includes the trunk-ground, crown-ground scattering.  The **G** component is the scattering directly back from ground attenuated by the forest canopy.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../../../nisar_leno_biomass_files/fig1.png\" alt=\"key.png\">  \n",
    "</div>\n",
    "\n",
    "<h3 style=\"text-align:center;\">\n",
    "  <strong>Fig 1:</strong> Scattering mechanisms of forest scattering model\n",
    "</h3>\n",
    "\n",
    "> The model can be written in the form of the following equation,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s^{\\beta_p})) \\quad\\quad\\quad & [D: Volume] \\\\\n",
    "               & + C_{p,t} W_s^{\\gamma_p} \\Gamma_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [I: Volume-Surface] \\\\\n",
    "               & + S_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [G: Surface]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> where the subscripts $p, s, t$ represents the variations over $polarization, space, time$.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "<a id=\"SEC_21\"></a>\n",
    "> ### 2.1. Basic Assumptions\n",
    ">The simplified assumptions here include: <br>\n",
    ">1. Parameters $A_{p,t}, B_{p,t}, C_{p,t}$ change over time and different polarizations, but vary little spatially within a local window.\n",
    ">2. Parameters $\\alpha_{p}, \\beta_{p}, \\gamma_{p}$ change with different polarizations, but vary little temporally, or spatially within a local window. \n",
    ">3. Parameters $\\Gamma_{p,t}, S_{p,t}$, related to both surface roughness and dielectric properties, change over time and different polarizations,but vary little spatially within a local window.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "<a id=\"SEC_22\"></a>\n",
    "> ### 2.2. Further Simplifications\n",
    "> We rewrite the original equation as\n",
    "\n",
    "> $$\n",
    "\\begin{align}\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s^{\\beta_p})) \\quad\\quad\\quad & [D] \\\\\n",
    "               & + C_{p,t} W_s^{\\gamma_p} \\Gamma_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [I] \\\\\n",
    "               & + S_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [G] \\\\\n",
    "\\Rightarrow \\quad\\quad\\quad \\quad\\quad\\quad  & \\\\\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s)) \\quad\\quad\\quad & [D] \\\\\n",
    "               & + C'_{p,t} S_{t} W_s^{\\gamma_p} \\exp(-B_{p,t} W_s)  \\quad\\quad\\quad & [I] \\\\\n",
    "               & + D_{p} S_{t} \\exp(-B_{p,t} W_s) \\quad\\quad\\quad & [G]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> where we have \n",
    "\n",
    ">**Assumption 1:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta_p & = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">This assumption is consistent with the estimation of forward model simulations.\n",
    "\n",
    ">**Assumption 2:**\n",
    "\n",
    ">$S$ term (backscattering crosssection from soil rough surface, no dependency on vegetation) is separated into polarization-dependent term and time-dependent term,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{p,t} & = D_{p} S_{t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ">**Assumption 3:**\n",
    "\n",
    ">$C\\Gamma$ term (reflectivity of the soil rough surface, and no dependency on vegetation) is rewritten as the new constant $C'$ with time-dependent $S_t$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C_{p,t} \\Gamma_{p,t} & = C_{p,t} K_{p} S_{t} \\\\\n",
    "                     & = C'_{p,t} S_{t} \n",
    "\\end{align}\n",
    "$$\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "\n",
    "<a id=\"SEC_23\"></a>\n",
    "> ### 2.3. Final Parameters for Retrieval Algorithm\n",
    "\n",
    ">Based on the above simplications, we have the following list of unknown parameters to solve:\n",
    "\n",
    "$$ A_{p,t}, B_{p,t}, C'_{p,t}, D_{p}, \\alpha_p, \\gamma_p, S_t, W_s $$\n",
    "\n",
    ">For possible solutions, we need to satisfy the following condition,\n",
    "\n",
    "$$ 3pt + 3p + t + s < pts $$ \n",
    "\n",
    ">For example, a 2-polarization system of 3x3-window retrieval needs at least 2 multi-temporal observations to satisfy the condition above.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe7cfe-91a5-45bc-a7aa-07d1443801d3",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3\"></a>\n",
    "## 3. Implementation of NISAR BIOMASS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f198f-f8e9-4686-852d-fa8c049183e8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../../../nisar_leno_biomass_files/Approach.png\" alt=\"key.png\">  \n",
    "</div>\n",
    "\n",
    "<h3 style=\"text-align:center;\">\n",
    "  <strong>Fig 2:</strong> Framework for NISAR calibration/validation Approach.\n",
    "</h3>\n",
    "\n",
    "    \n",
    "**Fig. 2** outlines the framework for NISAR calibration/validation approach. The NISAR biomass algorithm is based on time series observation using NISAR L-band calibrated backscatter measurements at HH and HV polarization. The datasets used in this study consists NISAR or NISAR Simulated time-series datasets , LiDAR AGB maps and Land Use/Land Cover (LULC) maps. The framework, downloads the GCOV datasets, performs resampling of datasets to master geometry to generate a stack of time-series SAR imageries at 100 m resoltuion. Also, the LiDAR AGB and LULC maps are resampled to align with master geometry. Once the data stack is generated, the data cleaning is performed to remove outlier from the SAR datastack and LiDAR AGB Map (resmapling, modeling error, forest change). Once pre-processing is performd the notebooks calculates initial model parameters of NISAR semi-emperical model (canopy model) using LiDAR Map and stack of time-series SAR imageries. An initial AGB map is generated using quadratic model for entire scene, which is used as an initial AGB value for AGB estimation over entire scene. Then the inital model parameters and AGB values along with the their bound values was used to retrived the AGB map for entire scene at pixel level. The accuracy of the map over calibration/ validation area was estimated and a valdiation report is generated. Finally, the L3 AGB map along with the associate auxially files are exported into cal/val database.\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "The NISAR algorithm will make use of high-resolution and time series backscatter observations at dual-polarizations (HH and HV) to estimate AGB by compensating for the effects of environmental changes (soil and vegetation moisture and phenology) and structure (vegetation and surface topography)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69594c8b-4a1c-49f1-b0ea-15c1c840bbbe",
   "metadata": {},
   "source": [
    "<a id=\"SEC_4\"></a>\n",
    "## [4. Example of NISAR BIOMASS Algorithm Calibration/Validation](#SEC_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5e289-e19f-4a8c-9e8a-fe3129303c15",
   "metadata": {},
   "source": [
    "### 4. 1. Data Downloading and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d59b83-068b-47e5-ac8c-bd32e3f89ec3",
   "metadata": {},
   "source": [
    "### Overview of Tasks Performed\n",
    "> 1. **Data Directory Setup** : A structured directory layout is established to organize the data into raw, processed, and output directories. This ensures that the workflow remains clean and modular as we download, preprocess, and store the data.\n",
    "> 2. **Data Streaming** : We implement functionality to stream the NISAR GCOV products from ASF or EARTHACCESS and save geotiff into the local directory. This step ensures that the data is readily available for further processing.\n",
    "> 3. **Image Statistics Computation** : In this section, we calculate basic image statistics (mean, standard deviation, min, max pixel values) to gain insights into the image data. These statistics can help identify potential issues with the images (such as inconsistent lighting or color distribution) and guide the selection of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff286114-a482-4c83-b975-ea26e53a7a28",
   "metadata": {},
   "source": [
    "### Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6d8a3-b77c-4289-900e-d15dd787874d",
   "metadata": {},
   "source": [
    "#### Load the required pre-defined packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891edf8-551c-4913-8938-f1cb4684e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import s3fs\n",
    "import getpass\n",
    "import requests\n",
    "import rasterio\n",
    "import subprocess\n",
    "import earthaccess\n",
    "import asf_search\n",
    "import glob\n",
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import asf_search as asf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from osgeo import gdal\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from shapely.geometry import box\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c6ff0-c185-4cf2-ad5f-d096d1b1152b",
   "metadata": {},
   "source": [
    "## Calibration Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c3be4-7da8-4e4a-b603-980052d8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = True\n",
    "if demo:\n",
    "    SITE = 'Leno'\n",
    "else:\n",
    "    SITE = input('What site?')\n",
    "    box_left, box_top, box_right, box_bottom = [input('Define AOI West boundary?: '),\n",
    "                                            input('Define AOI North boundary?: '),\n",
    "                                            input('Define AOI East boundary?: '),\n",
    "                                            input('Define AOI South boundary?: ')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc04d8-f041-4655-83e5-7660be954e9b",
   "metadata": {},
   "source": [
    "##### Change the below 3 cells to code and run if you are independently running the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03c953-efa1-4305-8e17-3413cf0c2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THE MAIN DIRECTORY IN WHICH THE DATABASE FOR PROCESSING WILL BE SETUP \n",
    "CUR_DIR =  Path(os.getcwd())  # GET THE CURRENT DIRECTORY\n",
    "print(CUR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118d76f-8740-4efa-90f0-cbdcd43e1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. THE MAIN DIRECTORY IN WHICH THE DATABASE FOR PROCESSING WILL BE SETUP \n",
    "MAIN_DIR = Path(CUR_DIR).parents[1]  # CHANGE THE PATH MAIN DIRECTORY\n",
    "print(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ece322-24a9-4176-b93b-ae8e5a125397",
   "metadata": {},
   "source": [
    "#### Load the required custom packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8eacb-63b2-4b36-80a0-74748ceb88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3. PATH TO THE DIRECTORY WHERE THE CUSTOM MODULES ARE STORED \n",
    "SCRIPT_DIR =  MAIN_DIR / Path('algorithm/bin/')\n",
    "print(SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb837df0-cf69-49dd-86d4-2375715be832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT THE PATH OF SCRIPT DIRECTORY TO SYSTEM PATH TO LOAD THE CUSTOM MODULES AND ROTUINES  \n",
    "sys.path.insert(1, str(SCRIPT_DIR))\n",
    "\n",
    "#### LOAD THE CUSTOM MODULES AND ROTUINES\n",
    "import functions as fn\n",
    "import model_radar as mr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362346ce-05f4-44d4-80d7-3c336475f80b",
   "metadata": {},
   "source": [
    "### SETUP THE DATA DIRECTORY STRUCTURE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf2483-9d19-4880-b706-73f40e7b630b",
   "metadata": {},
   "source": [
    "##### DATA DIRECTORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bba1a-2db6-44db-963f-f21cf6d451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = MAIN_DIR / Path('data_repo/data/nisar_simulated/' + SITE + '/')\n",
    "os.makedirs(DATA_DIR, exist_ok='True') # CREATES DATA DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47d8da-5567-4a0d-b9aa-3154f066fa53",
   "metadata": {},
   "source": [
    "##### PROCESSING DIRECTORIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0c96f-8ac3-45de-bdb2-75ab2d8cba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PROCESSING DIRECTORY \n",
    "PROCESS_DIR = MAIN_DIR / Path('data_repo/processing/nisar_simulated/' + SITE + '/')\n",
    "os.makedirs(PROCESS_DIR, exist_ok='True') # CREATES Processing DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f3fb6-cb2a-42ab-8c4c-ff7becdd9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### META DATA DIRECOTRY \n",
    "META_DIR = PROCESS_DIR / 'Metadata'\n",
    "os.makedirs(META_DIR, exist_ok='True') # CREATES META DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79552be8-fd78-4f6d-89eb-abadbbbddc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAR GCOV TIMESERIES GEOTIFF \n",
    "GCOV_DIR = PROCESS_DIR / 'GCOV/'\n",
    "os.makedirs(GCOV_DIR, exist_ok='True') # CREATES SAR TIMESERIES DIRECTORY IF NOT EXISTING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82703a0c-77bc-408d-856c-5cd01ad75d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESAMPLED SAR TIMESERIES GEOTIFF \n",
    "GCOV_STACKS_DIR = PROCESS_DIR / 'GCOV_STACKS/'\n",
    "os.makedirs(GCOV_STACKS_DIR, exist_ok='True') # CREATES RESAMPLED SAR TIMESERIES DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14e771-2f65-4636-8ad5-e057beeb880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESAMPLED LULC MAPS\n",
    "LULC_DIR = PROCESS_DIR / 'LULC'\n",
    "os.makedirs(LULC_DIR, exist_ok='True') # CREATES RESAMPLED LULC DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5313bec-7dc6-4788-a98a-fc9c9120b603",
   "metadata": {},
   "source": [
    "##### OUTPUT DIRECTORIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dacc85-52e1-4322-a6f9-c5257fe94a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUTPUT DIRECTORY \n",
    "OUTDIR_DIR = MAIN_DIR / Path('data_repo/output/nisar_simulated/' +  SITE + '/')\n",
    "os.makedirs(OUTDIR_DIR, exist_ok='True')  # CREATES OUTPUT DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e9345-3f38-48af-a9c4-301a5378c8e9",
   "metadata": {},
   "source": [
    "### Data Streaming \n",
    "The NISAR GCOV Products can be downloaded from the  NASA's Alaska Satellite Facility (ASF) or EarthExplorer repositary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af192f7-db2f-4b5f-9bf0-8f99e59b52d8",
   "metadata": {},
   "source": [
    "#### Query the Database for SAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11466e2-d02c-40da-bfff-fb2ca26f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo:\n",
    "    gcov_url = ('s3://nisar-public-ebd/ATBD/ecosystems/disturbance/sites/leno/GCOV/simulated/*')\n",
    "    s3 = s3fs.S3FileSystem(anon=True,endpoint_url='https://s3.us-west-1.wasabisys.com')\n",
    "    ALL_GCOV_DATA_LINKS = ['s3://' + k  for k in s3.glob(gcov_url)]\n",
    "    ALL_GCOV_DATA = [os.path.basename(x) for x in ALL_GCOV_DATA_LINKS]\n",
    "else:\n",
    "    auth = earthaccess.login()\n",
    "    # earthaccess.__store__.in_region = True ## Uncomment if using cloud and located in us-west-2\n",
    "    results = earthaccess.search_data(short_name = 'NISAR_L2_GCOV_BETA_V1', \n",
    "                                        # temporal = ('2023-07-01 00:00:00', '2023-08-31 23:59:59'), # can also specify by time\n",
    "                                        # granule_name = '*T00888*', # here we filter by files with CRID value of *T00888*\n",
    "                                        bounding_box = (box_left, box_bottom, box_right, box_top )\n",
    "                                       )\n",
    "    ALL_GCOV_DATA_LINKS = earthaccess.open(results)\n",
    "    ALL_GCOV_DATA = [os.path.basename(x.path) for x in ALL_GCOV_DATA_LINKS]\n",
    "print(\"number of available scenes:\", len(ALL_GCOV_DATA_LINKS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c79fb-8ee1-4c89-8940-66a852b89cb7",
   "metadata": {},
   "source": [
    "#### Stream and Convert the NISAR Simulated H5 Files to GEOTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18052d24-ed6e-413f-9b54-bbf49cff09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo: \n",
    "    indices = '0:5'\n",
    "else:\n",
    "    indices = input('which GCOV files should be used (see Section 1.1 list for numbers)? ex: 0-1, 0, 1, 5, 1:5 inclusive. You must choose at least 2')\n",
    "    \n",
    "if ':' in indices:\n",
    "    indices2 = list(range(int(indices.split(':')[0]), int(indices.split(':')[1])+1))\n",
    "elif '-' in indices:\n",
    "    indices2 = list(range(int(indices.split('-')[0]), int(indices.split('-')[1])+1))\n",
    "elif ',' in indices:\n",
    "    indices2 = []\n",
    "    num = indices.split(',')\n",
    "    for n in range(0,len(num)):\n",
    "        indices2.append(int(num[n]))\n",
    "else:\n",
    "    print('index type not recognized, please rerun the cell')\n",
    "        \n",
    "\n",
    "## If you don't want to use all of the images, choose which indices to use now. \n",
    "# indices = range(0,19)\n",
    "print(indices2)\n",
    "time_series_length = len(indices2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51de3c2-7b27-421e-8dfd-ac9feb638caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dates of observation (HH and HV):\")\n",
    "date_array = []\n",
    "SAR_images = []\n",
    "for ii in indices2:\n",
    "    if demo:\n",
    "        datestr = ALL_GCOV_DATA_LINKS[ii].split('/')[-1].split('_')[11]\n",
    "        SAR_images.append(ALL_GCOV_DATA_LINKS[ii])\n",
    "    else:\n",
    "        datestr = ALL_GCOV_DATA_LINKS[ii].path.split('/')[-1].split('_')[11]\n",
    "        SAR_images.append(ALL_GCOV_DATA_LINKS[ii])\n",
    "    date_obj = pd.to_datetime(datestr)\n",
    "    print('%03d %s' %(ii, date_obj))\n",
    "    date_array.append(date_obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d7d21-3ab4-46a8-b4a9-0f0d4305080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_DATA_LIST = []\n",
    "\n",
    "for image in SAR_images:\n",
    "    if demo:\n",
    "        F = h5py.File(s3.open(image, \"rb\"))\n",
    "        FILE_ID = os.path.basename(image)\n",
    "    else:\n",
    "        F = h5py.File(image, \"r\")\n",
    "        FILE_ID = os.path.basename(image.path)\n",
    "    print(FILE_ID)\n",
    "    DS_X = F['science/LSAR/GCOV/grids/frequencyA/xCoordinates'][()]      # returns as a h5py dataset object\n",
    "    DS_Y = F['science/LSAR/GCOV/grids/frequencyA/yCoordinates'][()]      # returns as a h5py dataset object\n",
    "    DS_EPSG = F['science/LSAR/GCOV/grids/frequencyA/projection'][()]\n",
    "    DS_HHHH = F['science/LSAR/GCOV/grids/frequencyA/HHHH'][()]  # returns as a numpy array\n",
    "    DS_HVHV = F['science/LSAR/GCOV/grids/frequencyA/HVHV'][()]  # returns as a numpy array\n",
    "    \n",
    "    print('Raster bounds: ',min(DS_X),max(DS_X),min(DS_Y),max(DS_Y))\n",
    "    print('X Size: ',DS_X.shape[0],' Y Size: ',DS_Y.shape[0])\n",
    "    print('Resolution: ', DS_Y[0] - DS_Y[1],'m')\n",
    "    print('')\n",
    "    meta = {'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': None, \n",
    "            'width': DS_X.shape[0], \n",
    "            'height': DS_Y.shape[0], \n",
    "            'count': 1, \n",
    "            'crs': rasterio.CRS.from_epsg(DS_EPSG[()]), \n",
    "            'transform': rasterio.Affine(DS_X[1] - DS_X[0], 0.0, DS_X[0], 0.0, DS_Y[1] - DS_Y[0], DS_Y[0]), \n",
    "            'tiled': False, \n",
    "            'interleave': 'band'}\n",
    "\n",
    "    with rasterio.open(GCOV_DIR / FILE_ID.replace('.h5', '_HHHH_power.tif'), 'w', **meta) as dst:\n",
    "        dst.write(DS_HHHH,indexes=1)    \n",
    "    with rasterio.open(GCOV_DIR / FILE_ID.replace('.h5', '_HVHV_power.tif'), 'w', **meta) as dst:\n",
    "        dst.write(DS_HVHV,indexes=1)    \n",
    "    SAR_DATA_LIST.append(GCOV_DIR / FILE_ID.replace('.h5', '_HVHV_power.tif'))\n",
    "    SAR_DATA_LIST.append(GCOV_DIR / FILE_ID.replace('.h5',  '_HHHH_power.tif'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d0e20-8ad9-4f85-977f-a72d3d795378",
   "metadata": {},
   "source": [
    "#### Get the Dates of the NISAR Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231eab4-4554-4cc9-82f8-bebfe3f62a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CREATE A METAFILE WITH LIST OF ALL PRODUCTS AND DATES AND SAVE\n",
    "DATES = [fn.date_from_filename(filename) for filename in  ALL_GCOV_DATA]  # GET THE DATE OF DATA ACQUISITION FOR EACH PRODUCT ID\n",
    "DATES = sorted(DATES) # SORT THE DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5814fe2-f58f-4fa6-9e1f-6baf2fdec8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT THE PRODUCT ID BASED ON DATES\n",
    "ALL_GCOV_DATA = sorted(ALL_GCOV_DATA, key=fn.date_from_filename)\n",
    "ALL_GCOV_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc516-62e6-45c1-8fba-daedd2696a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A LIST WITH DATES AND PRODUCT ID\n",
    "SAR_DATA_ID = []\n",
    "for BAND_INDEX in range(len(ALL_GCOV_DATA_LINKS)):\n",
    "    SAR_DATA_ID.append((DATES[BAND_INDEX], ALL_GCOV_DATA[BAND_INDEX]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9558c-f5cc-4205-8c69-c26c9b02a62d",
   "metadata": {},
   "source": [
    "#### Save the List of NISAR Simulated Data to the CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faef6c8-59a0-4a4c-bd9b-4edbe6f9d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT THE CREATED LIST TO DATAFRAME WITH HEADER\n",
    "SAR_DATA_ID = pd.DataFrame(data = SAR_DATA_ID, columns =['DATES', 'PRODUCT_ID']) \n",
    "# SAVE THE DATAFRAME TO META DIRECTORY \n",
    "SAR_DATA_ID.to_csv(META_DIR / Path(SITE + '_nisarsimulated_Data_List.csv'), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be539fe3-cee8-4476-9e92-3b981c2110e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY THE PRODUCT ID WITH DATES\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "SAR_DATA_ID.style.set_properties(**{'text-align': 'center'})\n",
    "display(SAR_DATA_ID) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c18d1e-e79e-4858-af8d-a3f94ca9c6a3",
   "metadata": {},
   "source": [
    "#### Create a reference file of 100 m X100 m resolution  \n",
    "To create L3 product at 100 m resolution, the reference image is generated to resample all products to 100 m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806c7f2-335f-4772-80ce-e293fc74e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE NISAR PRODUCTS ARE VALIDATED AT 1-ha (100 m X100 m) RESOLUTION\n",
    "INPUT_FILE     = SAR_DATA_LIST[0] # PATH TO THE INPUT FILE FOR CREATING REFERENCE IMAGE\n",
    "REFERENCE_FILE = GCOV_STACKS_DIR / Path(SITE + \"_100m_REF.tif\") # PATH OF THE CREATED REFERENCE IMAGE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d2c2a-4735-416b-907b-fb9fa4dc9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A GDAL EXPRESSION TO RESAMPLE THE INPUT FILE BY AVERAGING OVER 100 m X100 m  \n",
    "gdal_expression = f'gdal_translate -tr 100 100 -r average {INPUT_FILE} {REFERENCE_FILE}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ab1a3-854b-49a5-888c-53923cbea161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE GDAL EXPRESSSION THROUGH THE SHELL AND CAPTURE ITS OUTPUT\n",
    "subprocess.check_output(gdal_expression, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab858e7-1417-43bf-a9bb-d5a805cc6104",
   "metadata": {},
   "source": [
    "#### Resample all RTC SAR images to reference image geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51360d-9f40-4887-90b4-01493eab5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESAMPLE ALL RTC SAR DATA TO REFERENCE IMAGE AND STORE INTO \n",
    "SAR_RES_DATA_LIST = []\n",
    "for IN_FILE in SAR_DATA_LIST:\n",
    "    # RESAMPLED OUTPUT FILE NAME \n",
    "    OUT_FILE = GCOV_STACKS_DIR / os.path.basename(IN_FILE).replace(\".tif\", \"_100m.tif\")\n",
    "    # RESAMPLED INPUT FILE TO REFERENCE GEOMETRY \n",
    "    fn.raster_clip(REFERENCE_FILE, IN_FILE, OUT_FILE, resampling_method=\"average\")  \n",
    "    # APPEND THE RESAMPLED FILE NAME\n",
    "    SAR_RES_DATA_LIST.append(OUT_FILE) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b121e5f-521c-40b4-a311-e0fab3faa076",
   "metadata": {},
   "source": [
    "#### Save the List of Resampled NISAR Simulated ALOS2 Data to the CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce05f9-2de1-4dc3-8e4f-83cdbcb9a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_RES_DATA_ID = pd.DataFrame(data = SAR_RES_DATA_LIST, columns =['DATA_ID'])  # CONVERT THE RESAMPLED LIST TO DATAFRAME \n",
    "SAR_RES_DATA_ID.to_csv(META_DIR / Path(SITE + '_nisarsimulated_Band_List.csv'), index=False) # SAVE THE DATAFRAME TO META DIRECTORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20bbe4-25bd-4336-b12c-62d8eebe16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "display(SAR_RES_DATA_ID) # DISPLAY THE BANDS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82003a5-6780-4e69-a422-b42bdf51c27c",
   "metadata": {},
   "source": [
    "### Image Statistics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb00b0f-4140-4e10-9912-ff531cd7d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CALCULATE IMAGE STATISTICS \n",
    "NUM_SCENES = int(len(SAR_RES_DATA_LIST)/2)\n",
    "# INITIALIZE THE HH FILE LIST\n",
    "HH_FILES = []  \n",
    "# INITIALIZE THE HV FILE LIST\n",
    "HV_FILES = []  \n",
    "for NUM in range(NUM_SCENES):\n",
    "    # HH data \n",
    "    HV_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 0]) # APPEND THE HV FILES \n",
    "    # HV data \n",
    "    HH_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 1]) # APPEND THE HH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9da02b-c487-4c09-b88b-64e66956accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523484b6-86b4-4735-803a-22061638aa52",
   "metadata": {},
   "source": [
    "### HV Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcaa348-51e5-4ee2-b4e6-e56d1cba7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE HV IMAGE STATISTICS \n",
    "DF_HV = fn.image_statistics(HV_FILES, SITE, POL = 'HV') \n",
    "# WRITE THE DATAFRAME OF HV IMAGE STATISTICS TO CSV FILE \n",
    "DF_HV.to_csv(META_DIR / Path(SITE + \"_HV_STATS.csv\")) \n",
    "# DISPLAY THE HV STATISTICS\n",
    "display(DF_HV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49995abc-f797-4dbf-a315-3dd7fb6b3fb3",
   "metadata": {},
   "source": [
    "### HH Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1ba2d-d56b-4e94-b070-fd3329605419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE HH IMAGE STATISTICS\n",
    "DF_HH = fn.image_statistics(HH_FILES, SITE, POL = 'HH')   \n",
    "# WRITE THE DATAFRAME OF HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HH.to_csv(META_DIR / Path(SITE + \"_HH_STATS.csv\")) \n",
    "# DISPLAY THE HH STATISTICS\n",
    "display(DF_HH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ebcbc-5120-49b8-a16a-713d1042e150",
   "metadata": {},
   "source": [
    "### SAR Time Series Visualization and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbaeac5-d609-49c9-ac77-f899fe7ffa70",
   "metadata": {},
   "source": [
    "#### Plot NISAR Simulated ALOS2 HH and HV Polarization Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a5fc4-62e8-46df-9a83-6ea9d5f353cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PLOT EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES): #\n",
    "    plt.rcParams['font.family'] = 'serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "    plt.rcParams['font.serif'] = ['DejaVu Serif']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "    plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "    # CREATES A FIGURE AND A GRID FOR SUBPLOTS \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 11)) \n",
    "    \n",
    "    # LOAD THE HH IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HH = HH_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    GAMMA_HH[GAMMA_HH < 0.0001] = np.nan\n",
    "    GAMMA_HH = 10 * np.log10(GAMMA_HH) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HV = HV_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    GAMMA_HV[GAMMA_HV < 0.0001] = np.nan\n",
    "    GAMMA_HV = 10 * np.log10(GAMMA_HV) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # GET THE EXTENT OF THE IMAGE  \n",
    "    X_MIN, X_MAX, Y_MIN, Y_MAX = fn.GetExtent(HH_INFO)   \n",
    "    # GET THE DATE OF THE IMAGE IN DD-MM-YYYY FORMAT\n",
    "    DATE = SAR_DATA_ID.DATES.iloc[NUM].strftime(\"%d-%m-%Y\") \n",
    "    \n",
    "    # DISPLAYS THE HH IMAGE \n",
    "    im = ax[0].imshow(\n",
    "        GAMMA_HH,\n",
    "        cmap = \"gray\",\n",
    "        vmin = -15,\n",
    "        vmax = 0,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "    )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax[0].grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[0].set_title(\"HH Backscatter Image: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[0].set_xlabel('Easting (m)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[0].set_ylabel('Northing (m)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax[0].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax[0].yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADDS A COLORBAR \n",
    "    cbar = ax[0].figure.colorbar(im, ax=ax[0], shrink=0.6, orientation='horizontal', pad=0.1)\n",
    "    # CUSTOMIZING THE COLOR BAR LABEL \n",
    "    cbar.set_label('$dB$', labelpad=-35, x = 1.05, y = 1.1, rotation=0)\n",
    "    \n",
    "    # DISPLAYS THE HV IMAGE \n",
    "    im = ax[1].imshow(\n",
    "        GAMMA_HV,\n",
    "        cmap = \"gray\",\n",
    "        vmin = -20,\n",
    "        vmax = -5,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "    )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax[1].grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[1].set_title(\"HV Backscatter Image: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[1].set_xlabel('Easting (m)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[1].set_ylabel('Northing (m)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax[1].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax[1].yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADDS A COLORBAR \n",
    "    cbar = ax[1].figure.colorbar(im, ax=ax[1], shrink=0.6, orientation='horizontal', pad=0.1)\n",
    "    # CUSTOMIZING THE COLOR BAR LABEL \n",
    "    cbar.set_label('$dB$', labelpad=-35, x = 1.05, y = 1.1, rotation=0)\n",
    "    \n",
    "    # ADJUST HORIZONTAL AND VERTICAL SPACING BETWEEN SUBPLOTS \n",
    "    plt.subplots_adjust(wspace=0.35, hspace=0.25)\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(META_DIR / Path(SITE + \"_SAR_Backscatter_\" + DATE + \".png\"), dpi=600, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6097bf2-c8fa-4718-a69b-ae406be463ba",
   "metadata": {},
   "source": [
    "#### PLOT STATISTIC OF EACH SAR IMAGES FROM THE FILTERED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaec972-a2dc-4164-af8a-dce4d3af96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT STATISTICS OF EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES):\n",
    "    plt.rcParams['font.family'] = 'serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "    plt.rcParams['font.serif'] = ['DejaVu Serif']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "    plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "    # CREATES A FIGURE AND A GRID FOR SUBPLOTS \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6)) \n",
    "    \n",
    "    # LOAD THE HH IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HH = HH_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    GAMMA_HH[GAMMA_HH < 0.0001] = np.nan\n",
    "    GAMMA_HH = 10 * np.log10(GAMMA_HH) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HV = HV_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    GAMMA_HV[GAMMA_HV < 0.0001] = np.nan\n",
    "    GAMMA_HV = 10 * np.log10(GAMMA_HV) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # GET THE DATE OF THE IMAGE IN DD-MM-YYYY FORMAT\n",
    "    DATE = SAR_DATA_ID.DATES.iloc[NUM].strftime(\"%d-%m-%Y\")  \n",
    "    \n",
    "    #GAMMA_HH_CLIP   = GAMMA_HH[(GAMMA_HH > np.nanpercentile(GAMMA_HH, 5))  & (GAMMA_HH < np.nanpercentile(GAMMA_HH, 95))]\n",
    "    GAMMA_HH_CLIP = GAMMA_HH\n",
    "    #  CREATES A HISTOGRAM FOR HH SAR IMAGE \n",
    "    im = ax[0].hist(GAMMA_HH_CLIP.flatten(), bins = np.arange(-25, 5, 0.5), color = 'dodgerblue', alpha = 0.5)\n",
    "    #  PLOTS THE MEAN VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN']),   color='blue',    linestyle='-', linewidth=2, label = 'Mean')\n",
    "    #  PLOTS THE MEDIAN VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEDIAN']), color='red', linestyle='--', linewidth=2, label = 'Median')\n",
    "    #  PLOTS THE MEAN + SD VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN'] - DF_HH.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2, label = 'Mean ± SD')\n",
    "    #  PLOTS THE MEAN - SD VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN'] + DF_HH.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2)\n",
    "    #  PLOTS THE 1 PERCENTILE  VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(np.nanpercentile(GAMMA_HH, 5),  color = 'saddlebrown', linestyle='--',  linewidth=2, label = '5 Percentile')\n",
    "    #  PLOTS THE 99 PERCENTILE  VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(np.nanpercentile(GAMMA_HH, 95), color = 'gold', linestyle='--',  linewidth=2, label = '95 Percentile')\n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[0].set_title(\"HH Backscatter Image Statistics: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[0].set_xlabel('HH Backscattered power (dB)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[0].set_ylabel('Count',  fontweight = 'bold')\n",
    "    # SET THE XLIM \n",
    "    ax[0].set_xlim([-25, 5])\n",
    "    # ADD THE LEGEND  \n",
    "    ax[0].legend()\n",
    "    \n",
    "    #GAMMA_HV_CLIP  = GAMMA_HV[(GAMMA_HV > np.nanpercentile(GAMMA_HV, 5))  & (GAMMA_HV < np.nanpercentile(GAMMA_HV, 95))]\n",
    "    GAMMA_HV_CLIP  = GAMMA_HV\n",
    "    #  CREATES A HISTOGRAM FOR HV SAR IMAGE \n",
    "    im = ax[1].hist(GAMMA_HV_CLIP.flatten(), bins= np.arange(-30, 0, 0.5), color = 'dodgerblue', alpha = 0.5)\n",
    "    #  PLOTS THE MEAN VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN']),   color='blue',    linestyle='-', linewidth=2, label = 'Mean')\n",
    "    #  PLOTS THE MEDIAN VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEDIAN']), color='red', linestyle='--', linewidth=2, label = 'Median')\n",
    "    #  PLOTS THE MEAN + SD VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN'] - DF_HV.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2, label = 'Mean ± SD')\n",
    "    #  PLOTS THE MEAN - SD VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN'] + DF_HV.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2)\n",
    "    #  PLOTS THE 1 PERCENTILE  VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(np.nanpercentile(GAMMA_HV, 5),  color = 'saddlebrown', linestyle='--',  linewidth=2, label = '5 Percentile')\n",
    "    #  PLOTS THE 99 PERCENTILE  VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(np.nanpercentile(GAMMA_HV, 95), color = 'gold', linestyle='--',  linewidth=2, label = '95 Percentile')\n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[1].set_title(\"HV Backscatter Image Statistics: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[1].set_xlabel('HV Backscattered power (dB)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[1].set_ylabel('Count',  fontweight = 'bold')\n",
    "    # SET THE XLIM \n",
    "    ax[1].set_xlim([-30, 0])\n",
    "    # ADD THE LEGEND  \n",
    "    ax[1].legend()\n",
    "    \n",
    "    # ADJUST HORIZONTAL AND VERTICAL SPACING BETWEEN SUBPLOTS \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.25)\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(META_DIR / Path(SITE  + \"_SAR_Backscatter_Statistics_\" + DATE + \".png\"), dpi=600, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144292f-8b90-4c4e-b26d-bdb8db32e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FILTER THE DATA, CREATE METADATA FILE, VERSION NUMBER AND FILE NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77cd69-cb96-4731-ae79-6ef6d93338a3",
   "metadata": {},
   "source": [
    "#### Data Filtering: Specifying Exclusions\n",
    "To refine the dataset entering the processing chain, this function allows you to specify data records for exclusion. Please provide the corresponding row numbers for any data you wish to omit from the operation.\n",
    "> **Input Format Specifications**<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  The system accepts two syntax formats for defining the excluded rows:<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a). *Comma-Separated Values:* For non-contiguous rows, enter each individual row number separated by a comma.<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example: 3, 8, 14 (This will exclude rows 3, 8, and 14).<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b). *Hyphenated Ranges:* For a continuous block of rows, enter the starting row number followed by a hyphen and the ending row number.<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example: 3-5 (This will exclude rows 3, 4, and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432e49-18ba-4294-9634-861e24613324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILTER THE SAR DATASET \n",
    "DF = pd.read_csv(META_DIR / Path(SITE + '_nisarsimulated_Data_List.csv'))\n",
    "print(DF['DATES'])\n",
    "# REMOVE THE DATES YOU WANT TO EXCLUDE FROM THE PROCESSING \n",
    "DF_FILTERED = fn.filter_data(DF)\n",
    "\n",
    "# VIEW THE FILTERED DATALIST \n",
    "display(DF_FILTERED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0145101-c799-4029-aa86-f2beb848f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE VERSION NUMBER OF FILE IF PREVIOUS RUN EXISTS\n",
    "V_N = str(fn.get_version_number(str(META_DIR), OVERWRITE = True)).zfill(3)\n",
    "\n",
    "### CREATE THE NEW FILE NAME FOR THE CURRENT RUN\n",
    "FILE_NAME  = 'NISAR_L_3_40_A_AGB_'  + f\"{datetime.strptime(DF_FILTERED['DATES'].iloc[0], \"%Y-%m-%d\").date().month:02}\" + '_' + f\"{datetime.strptime(DF_FILTERED['DATES'].iloc[-1], \"%Y-%m-%d\").date().month:02}\" + '_' + f\"{datetime.strptime(DF_FILTERED['DATES'].iloc[-1], \"%Y-%m-%d\").date().year % 100:02}\" + '_' + SITE + '_' + f\"{V_N:03}\" + '_' + str(datetime.now().strftime(\"%Y%m%d\")) \n",
    "\n",
    "# SAVE THE LIST OF FILTERED SAR DATA LIST TO CSV\n",
    "DF_FILTERED.to_csv(META_DIR / Path(FILE_NAME + '_FILTERED_DATA.csv'), index=False)\n",
    "\n",
    "print('The Filename is ' + FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628555d3-34aa-41d0-b864-f4a2ca2a3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUTPUT DIRECTORY \n",
    "OUT_DIR = OUTDIR_DIR / FILE_NAME  \n",
    "os.makedirs(OUT_DIR, exist_ok='True')  # CREATES OUTPUT DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1518eac-99f4-47c8-b619-8a4bf9222e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_RES_DATA_LIST = [os.path.basename(x) for x in  list(pd.read_csv(META_DIR / Path(SITE + '_nisarsimulated_Band_List.csv'))['DATA_ID'])]# OPEN AND READ THE CSV FILE\n",
    "DF_FILTERED_LIST = [os.path.basename(x).split('.h5')[0] for x in DF_FILTERED['PRODUCT_ID'].tolist()]\n",
    "# Find strings containing any of the substrings\n",
    "SAR_RES_DATA_LIST = [GCOV_STACKS_DIR / s for s in SAR_RES_DATA_LIST if any(sub in s for sub in DF_FILTERED_LIST)]\n",
    "print((DF_FILTERED_LIST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613637d1-47a0-4e78-a240-7c7f8f57e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER THE HH STATISTICS\n",
    "DF_HV_FILTERED = DF_HV[DF_HV['BAND_NAME'].isin(DF_FILTERED_LIST)]\n",
    "# WRITE THE DATAFRAME OF FILTERED HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HV_FILTERED.to_csv(OUT_DIR / Path(FILE_NAME + \"_HV_STATS.csv\")) \n",
    "# VIEW THE FILTERED DATALIST \n",
    "display(DF_HV_FILTERED)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb56289-4b56-4097-b5b8-fdf4f9d22149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER THE HH STATISTICS\n",
    "DF_HH_FILTERED = DF_HH[DF_HH['BAND_NAME'].isin(DF_FILTERED_LIST)]\n",
    "# WRITE THE DATAFRAME OF FILTERED HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HH_FILTERED.to_csv(OUT_DIR / Path(FILE_NAME + \"_HH_STATS.csv\")) \n",
    "# VIEW THE FILTERED DATALIST \n",
    "display(DF_HH_FILTERED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c418a2-8132-474b-943d-e78432900cbf",
   "metadata": {},
   "source": [
    "### 4. 2. Create Forest/Non-Forest Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27625140-e5a5-4dda-997b-3d65a4d72afe",
   "metadata": {},
   "source": [
    "### Overview of Tasks Performed\n",
    "> 1. **Create Forest/Non-Forest Mask** : In this section, we will create a Forest/Non-Forest Mask to distinguish forested areas from non-forested regions based on specific criteria. The process involves leveraging NLCD land cover data, SAR backscatter values, and LiDAR AGB value to accurately classify pixels as either forest or non-forest. Once the mask is created, it can be applied to filter the data and model forest biomass.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fae19-5033-44e8-9ebf-5cae5ebd4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of LULC Data\n",
    "LULC_YYYY = 2021 \n",
    "# Name of LULC Data\n",
    "LULC_NAME = 'NLCD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976df957-f79a-4058-9a39-0fbd2af8b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_LULC_FILE_LINK = 'https://s3.us-west-1.wasabisys.com/nisar-public-ebd/ATBD/ecosystems/disturbance/sites/leno/LENO_LULC.tif'\n",
    "# Path to directory in which LULC is saved as geotiff\n",
    "IN_LULC_FILE = LULC_DIR / Path(os.path.basename(IN_LULC_FILE_LINK))\n",
    "# Stream and save directly to file\n",
    "with requests.get(IN_LULC_FILE_LINK, stream=True) as r:\n",
    "    r.raise_for_status()  # raises error if status != 200\n",
    "    with open(IN_LULC_FILE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "# OUTPUT PATH TO THE RESAMPLED LULC MAP\n",
    "OUT_LULC_FILE = LULC_DIR  / Path(LULC_NAME + '_' + str(LULC_YYYY) + '_' + SITE + '_100m.tif')\n",
    "# RESAMPLE THE LULC MAP FILE BY TAKING MODE OVER 100 m X100 m GRID OF REFERENCE IMAGE\n",
    "fn.raster_clip(REFERENCE_FILE, IN_LULC_FILE, OUT_LULC_FILE, resampling_method = \"mode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a11799-13d3-4781-ba17-0c6ef02e1bba",
   "metadata": {},
   "source": [
    "##### Plot resampled NLCD Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ea320-8f43-4bd6-afd1-7b1d71cc1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PLOT LULC IMAGE\n",
    "DATA_INFO = gdal.Open(OUT_LULC_FILE)\n",
    "# READ AGB VALUES TO ARRAY \n",
    "LULC =  DATA_INFO.GetRasterBand(1).ReadAsArray()\n",
    "# GET THE NLCD PALATTE \n",
    "HF = fn.get_color_palette_nlcd()\n",
    "# CREATE A CUSTOM COLORMAP\n",
    "CMAP = mpl.colors.ListedColormap(list(HF['Palette'].values))\n",
    "\n",
    "\n",
    "\n",
    "NORM = mpl.colors.BoundaryNorm(list(HF['Value'].values), ncolors=len(list(HF['Palette'].values)))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "# CREATES A FIGURE \n",
    "fig, ax = plt.subplots(figsize=(14, 6)) \n",
    "# DISPLAYS THE LULC IMAGE\n",
    "im = plt.imshow(\n",
    "    LULC,\n",
    "    cmap = CMAP,\n",
    "    norm = NORM, \n",
    "    extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    ")\n",
    "# ADD THE GRID TO IMAGE \n",
    "ax.grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "# ADD THE TITLE TO IMAGE \n",
    "ax.set_title(LULC_NAME + \" \" + str(LULC_YYYY) + \" Map\", fontweight = 'bold')\n",
    "# ADD THE XLABEL TO IMAGE \n",
    "ax.set_xlabel('Longitude (Deg)', fontweight = 'bold')\n",
    "# ADD THE YLABEL TO IMAGE \n",
    "ax.set_ylabel('Latitude (Deg)',  fontweight = 'bold')\n",
    "# FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# CREATE HANDLES FOR LEGEND\n",
    "handles = []\n",
    "for index, row in HF.iterrows():\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', label=row['Description'],\n",
    "                                markerfacecolor=row['Palette'], markersize=10))\n",
    "# DISPLAY THE LEGEND\n",
    "ax.legend(handles=handles, title=\"Land Cover Classes\", loc='lower right',\n",
    "          bbox_to_anchor=(1.3, 0), fontsize=8, borderaxespad=0.)\n",
    "# ADJUST PADDING AROUND THE IMAGE\n",
    "plt.tight_layout()\n",
    "# SAVE THE FIGURE \n",
    "plt.savefig(OUT_DIR  / Path(SITE + \"_LULC.png\"), dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9622b62-a054-41f5-a7a5-6a4f93589278",
   "metadata": {},
   "source": [
    "##### Create the mask file with  AGB pixels greater than 0 Mg/ha and valid HH and HV observations \n",
    "\n",
    "1. Compute the minimum (HHmin, Hvmin) and maximum (HHmax, Hvmax) values of the backscatter for every pixel in the image over the entire time series.\n",
    "2. Generate a backscatter mask by selecting pixels that have an HVmin value greater than the threshold value, and an HHmax and HVmax value less than 1.\n",
    "3. Utilize the Landcover data to create a Land Use/Land Cover (LULC) mask that excludes water, man-made, and non-vegetation classes. \n",
    "4. Use the LIDAR AGB Map to create a mask that excludes non-AGB values. \n",
    "4. Finally, combine the backscatter mask, LULC mask, and X mask to generate the FNF mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039ed8f-095f-4c61-a376-18086ccf362f",
   "metadata": {},
   "source": [
    "##### Load the SAR data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b066c-6273-49ac-801d-01e1ee861634",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_RES_DATA_LIST = [os.path.basename(x) for x in  list(pd.read_csv(META_DIR / Path(SITE + '_nisarsimulated_Band_List.csv'))['DATA_ID'])]# OPEN AND READ THE CSV FILE\n",
    "DF_FILTERED = pd.read_csv(META_DIR / Path(FILE_NAME + '_FILTERED_DATA.csv'))\n",
    "DF_FILTERED_LIST = [os.path.basename(x).split('.h5')[0] for x in DF_FILTERED['PRODUCT_ID'].tolist()]\n",
    "# Find strings containing any of the substrings\n",
    "SAR_RES_DATA_LIST = [GCOV_STACKS_DIR / s for s in SAR_RES_DATA_LIST if any(sub in s for sub in DF_FILTERED_LIST)]\n",
    "\n",
    "HH_FILES = []  # INITIALIZE THE HH FILE LIST\n",
    "HV_FILES = []  # INITIALIZE THE HV FILE LIST\n",
    "# CALCULATE NUMBER OF SCENES\n",
    "NUM_SCENES = int(len(SAR_RES_DATA_LIST)/2)\n",
    "\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # HH data \n",
    "    HV_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 0]) # APPEND THE HV FILES \n",
    "    # HV data \n",
    "    HH_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 1]) # APPEND THE HH FILES \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576ccca-002f-46d7-881a-5c7cb2040a14",
   "metadata": {},
   "source": [
    "##### Create Minimum and Maximum Backscatter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77512c68-842d-42a5-85b1-5b560edfbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE HV MIN AND MAX LAYER \n",
    "DA_HV = []\n",
    "# PLOT STATISTICS OF EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HV = HV_INFO.GetRasterBand(1).ReadAsArray() # READ THE HH DATA INTO ARRAY  \n",
    "    DA_HV.append(GAMMA_HV) # APPEND THE HV LAYER\n",
    "    \n",
    "HV_MIN = np.min(np.array(DA_HV), axis=0) # ESTIMATE MIN OF THE HV LAYER ACROSS TIMESERIES\n",
    "HV_MAX = np.max(np.array(DA_HV), axis=0) # ESTIMATE MAX OF THE HV LAYER ACROSS TIMESERIES\n",
    "\n",
    "\n",
    "# CREATE HH MIN AND MAX LAYER \n",
    "DA_HH = []\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # LOAD THE HV IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HH = HH_INFO.GetRasterBand(1).ReadAsArray() # READ THE HH DATA INTO ARRAY  \n",
    "    DA_HH.append(GAMMA_HH) # APPEND THE HH LAYER\n",
    "    \n",
    "HH_MIN = np.min(np.array(DA_HH), axis=0) # ESTIMATE MIN OF THE HH LAYER ACROSS TIMESERIES\n",
    "HH_MAX = np.max(np.array(DA_HH), axis=0) # ESTIMATE MAX OF THE HH LAYER ACROSS TIMESERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b467b8e-568c-4c7c-af96-bbdb20447fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SAR MASK WITH VALID HH AND HV VALUES FROM MIN MAX LAYERS \n",
    "SAR_MASK = np.where((HV_MIN > 0.01) & (HV_MAX < 1) & (HH_MAX < 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167db7e-bc24-403b-9369-7ca8c9f9e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A LULC MASK\n",
    "LULC_MASK = (LULC != 11) & (LULC != 12) &  (LULC != 22) & (LULC != 23) & (LULC != 24)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4f2ed-64a5-466c-9364-7f7dd493a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FNF MASK\n",
    "MASK = np.where((SAR_MASK == 1) & (LULC_MASK == 1), 1, 0)\n",
    "MASK_FILE = OUT_DIR / Path(FILE_NAME + \"_mask_100m.tif\") # CREATE A PATH TO SAVE THE MASK LAYER\n",
    "fn.write_geotiff_with_gdalcopy(REFERENCE_FILE, MASK, MASK_FILE) # SAVE THE MASK FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f99ec-63c4-46d8-80c3-cee2e66fa648",
   "metadata": {},
   "source": [
    "### 4. 3. Loading Calibrated Model & initial AGB Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf24dd6-127a-44ba-87fd-4c578e9f5358",
   "metadata": {},
   "source": [
    "### Overview of Tasks Performed\n",
    "> 1. **Initial Aboveground Biomass Estimation** : Initial estimates of AGB was estimated. Here HV polarizations values are used to estimate AGB values. \n",
    "> 2. **Load the NISAR Biomass Model Parameters** : The initial estimate of NISAR biomass model parameters is a crucial step in developing accurate predictive model. In this study, initial estimates of the NISAR biomass model parameters are derived from the mean values of time series data gathered at the calibration site. These mean values, which capture the average trends and fluctuations of backscatter over time, provide a strong starting point for model calibration for time series estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150728bd-b087-4644-97bc-cd5399154dc0",
   "metadata": {},
   "source": [
    "### Estimate the initial AGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77cecb-1aee-4239-acf9-ddbf9122c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT PATH TO THE FNF MAP\n",
    "MASK_FILE_100 = OUT_DIR / Path(FILE_NAME + \"_mask_100m.tif\")\n",
    "# Estimate the initial AGB map\n",
    "mr.initial_agb_estimation(OUT_DIR, MASK_FILE, MASK_FILE, None, SAR_RES_DATA_LIST, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71911635-188b-4f7f-a939-d497b044895d",
   "metadata": {},
   "source": [
    "### Load the calibration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5344f-d616-406f-a0c8-572e73fae9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configparser object\n",
    "CONFIG = configparser.ConfigParser()\n",
    "\n",
    "# Read the ini file\n",
    "CONFIG.read(MAIN_DIR / Path('config/config.ini'))\n",
    "\n",
    "\n",
    "PARAM  = CONFIG['NISAR'] \n",
    "PARAM0_FILE = OUT_DIR / Path(FILE_NAME + \"_model_sim_param0_s13.csv\")\n",
    "\n",
    "\n",
    "params = np.array([float(PARAM['AHV']), float(PARAM['AHH']), \n",
    "                    float(PARAM['BHV']), float(PARAM['BHH']),\n",
    "                    float(PARAM['CHV']), float(PARAM['CHH']),\n",
    "                    float(PARAM['alphaHV']), float(PARAM['alphaHH']),\n",
    "                    float(PARAM['deltaHV']), float(PARAM['deltaHH']),\n",
    "                    float(PARAM['DHV']), float(PARAM['DHH']),\n",
    "                    float(PARAM['S'])])\n",
    "\n",
    "\n",
    "param_names = [\n",
    "                  \"A_HV\",\n",
    "                  \"A_HH\",\n",
    "                  \"B_HV\",\n",
    "                  \"B_HH\",\n",
    "                  \"C_HV\",\n",
    "                  \"C_HH\",\n",
    "                  \"alpha_HV\",\n",
    "                  \"alpha_HH\",\n",
    "                  \"gamma_HV\",\n",
    "                  \"gamma_HH\",\n",
    "                  \"D_HV\",\n",
    "                  \"D_HH\",\n",
    "              ] + [\"S\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(params[:, None].T, columns=param_names)\n",
    "df.to_csv(PARAM0_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352fdd0-51cb-4926-a037-eeb897912c53",
   "metadata": {},
   "source": [
    "### 4. 4. L3 Biomass Product Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0009e6-a17d-482e-a1e9-f67282398109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3a42-22c1-4b87-b5cc-83b5a49bdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.colormap_plot_agb_prediction(OUT_DIR, SAR_RES_DATA_LIST, MASK_FILE, PARAM0_FILE, FILE_NAME, a_max=300, ab_range=[45, 51])\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cb0f1-7a57-4fac-996a-f37af65bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup Metadata Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95870558-bd96-4b17-94d8-07832da46d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bccaac-4e06-439d-ad4b-032de2056074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.local-NISAR_Biomass]",
   "language": "python",
   "name": "conda-env-.local-NISAR_Biomass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
