{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ed4451-b706-4c67-88d0-635d35fc35a0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../../nisar_leno_biomass_files/NISAR_Mission_Logo.png\" alt=\"key.png\">  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614a824-39d2-4a9e-a4ad-ba28ab63b5a9",
   "metadata": {},
   "source": [
    "# NASA ISRO Synthetic Aperture Radar Mission\n",
    "<br>\n",
    "\n",
    "## Algorithm Theoretical Basis Document and Jupyter Notebook for NISAR Biomass Retrieval\n",
    "<br>\n",
    "\n",
    "### Authors: \n",
    "> Naveen Ramachandran, KC Cushman, Alexandra Christensen, & Sassan Saatchi <br>\n",
    "> Carbon Cycle and Ecosystems <br>\n",
    "> Jet Propulsion Laboratory <br>\n",
    "> California Institute of Technology <br>\n",
    "> Date: 2025-07-18 <br>\n",
    "\n",
    "### Summary\n",
    "> This notebook describes the algorithm theoretical basis document (ATBD) for retrieving forest aboveground biomass (AGB) from a NISAR simulated time-series data stacks. The algorithm is designed to meet the Level 3 Science requirements for generating AGB product annually. This notebook constitutes a combination of formulating the theoretical basis for the NISAR Forest Biomass algorithm that is based on semi-empirical model and an implementation of the algorithm in executable python code. \n",
    "\n",
    "> A test data set accompanies the notebook that is available from NASA's Alaska Satellite Facility (ASF) and EarthExplorer and referenced in this notebook. Also accomanying the notebook is a python code library module (`atbd_biomass.py`) that is imported into this notebook in order for the example of the algorithm to execute within the notebook.\n",
    "\n",
    "> In this notebook, we will walk through the various steps involved for estimation of NISAR L3 AGB product from NISAR simulated UAVSAR data. To illustrate the usage of NISAR AGB Model, we will use a NISAR simulated ALOS2 datasets acquired during NISAR Calibration/Validation phase.\n",
    "\n",
    "> This ATBD notebook consists of following tasks:- \n",
    "> 1. **Data Downloading and Analysis** : This step performs data downloading and estimation of image statistics in HH and HV Polarization.\n",
    "> 2. **Create Forest/Non-Forest Mask** : This steo generates FNF mask the SAR scene.\n",
    "> 3. **Loading Calibrated Model & initial AGB Estimation** : This step loads the calibration parameters of NISAR biomass algorithm and estimates initial AGB values. \n",
    "> 4. **L3 Biomass Product Generation** : This str[ produces L3 Biomass Map over the entire sites.\n",
    "\n",
    "> **The NISAR requirement for BIOMASS product is** *to measure above ground woody vegetation biomass annually at the hectare scale (1 ha) to an RMS accuracy of 20 Mg/ha for 80% of areas of biomass less than 100 Mg/ha.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8ea34-ffaa-4b97-9744-23ad7d74e1ae",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>\n",
    "## Table of Contents\n",
    "1. [Context on Forest Biomass and NISAR Time series](#section-1)\n",
    "2. [Canopy Model for Radar Backscattering](#SEC_2) <br>\n",
    "    2.1. [Basic Assumptions](#SEC_21) <br>\n",
    "    2.2. [Further Simplifications](#SEC_22) <br>\n",
    "    2.3. [Final Parameters for Retrieval Algorithm](#SEC_23) <br>\n",
    "3. [Implementation of NISAR BIOMASS Algorithm](#SEC_3)\n",
    "4. [Example of NISAR BIOMASS Algorithm Calibration/Validation](#SEC_4) <br>\n",
    "    4.1. [Execute Notebook 0: Data Downloading and Analysis](#SEC_41) <br>\n",
    "    4.2. [Execute Notebook 1: Create Calibration / Validation Products](#SEC_42) <br>\n",
    "    4.3. [Execute Notebook 2: NISAR Model Calibration & Validation](#SEC_43) <br>\n",
    "    4.4. [Execute Notebook 3: L3 Biomass Product Generation](#SEC_44) <br>\n",
    "    4.5. [Execute Notebook 4: Uploading L3 Biomass products to Cal./Val and/or S3 Database](#SEC_45) <br>\n",
    "5. [References](#SEC_5) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5413a-6583-40a3-8a5d-d4dce74a9599",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "## 1. Context on Forest Biomass and NISAR Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeee2d-efd2-49dd-866c-55157e5b6b0a",
   "metadata": {},
   "source": [
    "> Forest aboveground biomass (AGB) is a underlying parameter for predicting the spatial distribution of carbon in the terrestrial ecosystem. Also, it is vital from an economic (assits food, timber, and energy industry), disaster (forest fire, etc.), soil (soil errosion, landslide, etc.), water (flooding, water quality, etc.) biodoversity management (Foley et al. 2005; Chazdon 2008). However, the current knowledge of the distribution and magnitude of terrestrial biomass is based almost entirely on ground measurements over an extremely small, and possibly biased sample, with almost no measurements in the southern hemisphere and equatorial regions (Schimel et al., 2015). Forest AGB is directly related to the structural attributes of the forest. Synthetic Aperture Radar (SAR) backscatter measurements are sensitive to forest strctural information. Hence it can be correlated to AGB. However, the SAR backscatter sensitivity to AGB varies depending on the wavelength and geometry of the radar measurements, and is influenced by the surface topography, structure of vegetation, and environmental conditions such as soil moisture and vegetation phenology or moisture.\n",
    "\n",
    "> For the NISAR mission, the focus was on AGB values between 0- 100 Mg/ha, as the sensitivity of L-band backscatter measurements to AGB saturates around 100 Mg/ha (Yu Lin 2016), which covers 50% of the global forests and the entire area of other woody vegetation (FRA, 2010). Also, with wide swath (240 km), high resolution (~15m), 12-day repeat orbit cycle, it allow retrieval of AGB at high spatial resolution over global scale temporally, allowing us to understand the dynamics of forest ecosystem. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bdf31-badf-461d-8028-0f51218246e8",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2\"></a>\n",
    "## 2. Canopy Model for Radar Backscattering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610b664-938a-4f18-8cda-f60bd59fdb4b",
   "metadata": {},
   "source": [
    "\n",
    ">The physical based data-fitted model (Saatchi and McDonald, 1997) describes scattering mechanisms of the forest with three  components **(Fig. 1)**, namely direct **D** , Interactive **I** , i.e., direct-reflect, and ground **G** components.  \n",
    "\n",
    ">The **D** component consists of the scattering from trunks, branches, and leaves directly back to the sensor.  The **I** terms, also known as \"double bounce\" term, is the scattering that includes interaction between trees and ground.  This includes the trunk-ground, crown-ground scattering.  The **G** component is the scattering directly back from ground attenuated by the forest canopy.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../../nisar_leno_biomass_files/fig1.png\" alt=\"key.png\">  \n",
    "</div>\n",
    "\n",
    "\n",
    "<h3 style=\"text-align:center;\">\n",
    "  <strong>Fig 1:</strong> Scattering mechanisms of forest scattering model\n",
    "</h3>\n",
    "\n",
    "> The model can be written in the form of the following equation,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s^{\\beta_p})) \\quad\\quad\\quad & [D: Volume] \\\\\n",
    "               & + C_{p,t} W_s^{\\gamma_p} \\Gamma_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [I: Volume-Surface] \\\\\n",
    "               & + S_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [G: Surface]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> where the subscripts $p, s, t$ represents the variations over $polarization, space, time$.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "<a id=\"SEC_21\"></a>\n",
    "> ### 2.1. Basic Assumptions\n",
    ">The simplified assumptions here include: <br>\n",
    ">1. Parameters $A_{p,t}, B_{p,t}, C_{p,t}$ change over time and different polarizations, but vary little spatially within a local window.\n",
    ">2. Parameters $\\alpha_{p}, \\beta_{p}, \\gamma_{p}$ change with different polarizations, but vary little temporally, or spatially within a local window. \n",
    ">3. Parameters $\\Gamma_{p,t}, S_{p,t}$, related to both surface roughness and dielectric properties, change over time and different polarizations,but vary little spatially within a local window.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "<a id=\"SEC_22\"></a>\n",
    "> ### 2.2. Further Simplifications\n",
    "> We rewrite the original equation as\n",
    "\n",
    "> $$\n",
    "\\begin{align}\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s^{\\beta_p})) \\quad\\quad\\quad & [D] \\\\\n",
    "               & + C_{p,t} W_s^{\\gamma_p} \\Gamma_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [I] \\\\\n",
    "               & + S_{p,t} \\exp(-B_{p,t} W_s^{\\beta_p})  \\quad\\quad\\quad & [G] \\\\\n",
    "\\Rightarrow \\quad\\quad\\quad \\quad\\quad\\quad  & \\\\\n",
    "\\sigma_0(p,s,t) & = A_{p,t} W_s^{\\alpha_p} (1 - \\exp(-B_{p,t} W_s)) \\quad\\quad\\quad & [D] \\\\\n",
    "               & + C'_{p,t} S_{t} W_s^{\\gamma_p} \\exp(-B_{p,t} W_s)  \\quad\\quad\\quad & [I] \\\\\n",
    "               & + D_{p} S_{t} \\exp(-B_{p,t} W_s) \\quad\\quad\\quad & [G]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> where we have \n",
    "\n",
    ">**Assumption 1:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta_p & = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">This assumption is consistent with the estimation of forward model simulations.\n",
    "\n",
    ">**Assumption 2:**\n",
    "\n",
    ">$S$ term (backscattering crosssection from soil rough surface, no dependency on vegetation) is separated into polarization-dependent term and time-dependent term,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{p,t} & = D_{p} S_{t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ">**Assumption 3:**\n",
    "\n",
    ">$C\\Gamma$ term (reflectivity of the soil rough surface, and no dependency on vegetation) is rewritten as the new constant $C'$ with time-dependent $S_t$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C_{p,t} \\Gamma_{p,t} & = C_{p,t} K_{p} S_{t} \\\\\n",
    "                     & = C'_{p,t} S_{t} \n",
    "\\end{align}\n",
    "$$\n",
    "> [back to TOC](#TOC)\n",
    "\n",
    "\n",
    "<a id=\"SEC_23\"></a>\n",
    "> ### 2.3. Final Parameters for Retrieval Algorithm\n",
    "\n",
    ">Based on the above simplications, we have the following list of unknown parameters to solve:\n",
    "\n",
    "$$ A_{p,t}, B_{p,t}, C'_{p,t}, D_{p}, \\alpha_p, \\gamma_p, S_t, W_s $$\n",
    "\n",
    ">For possible solutions, we need to satisfy the following condition,\n",
    "\n",
    "$$ 3pt + 3p + t + s < pts $$ \n",
    "\n",
    ">For example, a 2-polarization system of 3x3-window retrieval needs at least 2 multi-temporal observations to satisfy the condition above.\n",
    "\n",
    "> [back to TOC](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe7cfe-91a5-45bc-a7aa-07d1443801d3",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3\"></a>\n",
    "## 3. Implementation of NISAR BIOMASS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f198f-f8e9-4686-852d-fa8c049183e8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../../nisar_leno_biomass_files/Approach.png\" alt=\"key.png\">  \n",
    "</div>\n",
    "\n",
    "<h3 style=\"text-align:center;\">\n",
    "  <strong>Fig 2:</strong> Framework for NISAR calibration/validation Approach.\n",
    "</h3>\n",
    "\n",
    "    \n",
    "**Fig. 2** outlines the framework for NISAR calibration/validation approach. The NISAR biomass algorithm is based on time series observation using NISAR L-band calibrated backscatter measurements at HH and HV polarization. The datasets used in this study consists NISAR or NISAR Simulated time-series datasets , LiDAR AGB maps and Land Use/Land Cover (LULC) maps. The framework, downloads the GCOV datasets, performs resampling of datasets to master geometry to generate a stack of time-series SAR imageries at 100 m resoltuion. Also, the LiDAR AGB and LULC maps are resampled to align with master geometry. Once the data stack is generated, the data cleaning is performed to remove outlier from the SAR datastack and LiDAR AGB Map (resmapling, modeling error, forest change). Once pre-processing is performd the notebooks calculates initial model parameters of NISAR semi-emperical model (canopy model) using LiDAR Map and stack of time-series SAR imageries. An initial AGB map is generated using quadratic model for entire scene, which is used as an initial AGB value for AGB estimation over entire scene. Then the inital model parameters and AGB values along with the their bound values was used to retrived the AGB map for entire scene at pixel level. The accuracy of the map over calibration/ validation area was estimated and a valdiation report is generated. Finally, the L3 AGB map along with the associate auxially files are exported into cal/val database.\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "The NISAR algorithm will make use of high-resolution and time series backscatter observations at dual-polarizations (HH and HV) to estimate AGB by compensating for the effects of environmental changes (soil and vegetation moisture and phenology) and structure (vegetation and surface topography)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69594c8b-4a1c-49f1-b0ea-15c1c840bbbe",
   "metadata": {},
   "source": [
    "<a id=\"SEC_4\"></a>\n",
    "## [4. Example of NISAR BIOMASS Algorithm Calibration/Validation](#SEC_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5e289-e19f-4a8c-9e8a-fe3129303c15",
   "metadata": {},
   "source": [
    "### 4. 1. Data Downloading and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d59b83-068b-47e5-ac8c-bd32e3f89ec3",
   "metadata": {},
   "source": [
    "### Overview of Tasks Performed\n",
    "> 1. **Data Directory Setup** : A structured directory layout is established to organize the data into raw, processed, and output directories. This ensures that the workflow remains clean and modular as we download, preprocess, and store the data.\n",
    "> 2. **Data Streaming** : We implement functionality to stream the NISAR GCOV products from ASF or EARTHACCESS and save geotiff into the local directory. This step ensures that the data is readily available for further processing.\n",
    "> 3. **Image Statistics Computation** : In this section, we calculate basic image statistics (mean, standard deviation, min, max pixel values) to gain insights into the image data. These statistics can help identify potential issues with the images (such as inconsistent lighting or color distribution) and guide the selection of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff286114-a482-4c83-b975-ea26e53a7a28",
   "metadata": {},
   "source": [
    "### Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6d8a3-b77c-4289-900e-d15dd787874d",
   "metadata": {},
   "source": [
    "#### Load the required pre-defined packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891edf8-551c-4913-8938-f1cb4684e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import s3fs\n",
    "import getpass\n",
    "import requests\n",
    "import rasterio\n",
    "import subprocess\n",
    "import earthaccess\n",
    "import asf_search\n",
    "import glob\n",
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import asf_search as asf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from osgeo import gdal\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from shapely.geometry import box\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c6ff0-c185-4cf2-ad5f-d096d1b1152b",
   "metadata": {},
   "source": [
    "## Calibration Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03c953-efa1-4305-8e17-3413cf0c2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THE MAIN DIRECTORY IN WHICH THE DATABASE FOR PROCESSING WILL BE SETUP \n",
    "CUR_DIR =  Path(os.getcwd())  # GET THE CURRENT DIRECTORY\n",
    "print(CUR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118d76f-8740-4efa-90f0-cbdcd43e1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. THE MAIN DIRECTORY IN WHICH THE DATABASE FOR PROCESSING WILL BE SETUP \n",
    "MAIN_DIR = Path(CUR_DIR).parents[1]  # CHANGE THE PATH MAIN DIRECTORY\n",
    "print(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ece322-24a9-4176-b93b-ae8e5a125397",
   "metadata": {},
   "source": [
    "#### Load the required custom packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8eacb-63b2-4b36-80a0-74748ceb88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3. PATH TO THE DIRECTORY WHERE THE CUSTOM MODULES ARE STORED \n",
    "SCRIPT_DIR =  MAIN_DIR / Path('algorithm/bin/')\n",
    "print(SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb837df0-cf69-49dd-86d4-2375715be832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT THE PATH OF SCRIPT DIRECTORY TO SYSTEM PATH TO LOAD THE CUSTOM MODULES AND ROTUINES  \n",
    "sys.path.insert(1, str(SCRIPT_DIR))\n",
    "\n",
    "#### LOAD THE CUSTOM MODULES AND ROTUINES\n",
    "import functions as fn\n",
    "import model_radar as mr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccafed-abde-4c21-903e-7c70b58b836d",
   "metadata": {},
   "source": [
    "#### Query the Cal/Val database for Cal/Val site Info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c3be4-7da8-4e4a-b603-980052d8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO = True\n",
    "if DEMO:\n",
    "    SITE = 'LENO'\n",
    "else:\n",
    "    SITE = input('What site?')\n",
    "    BOX_LEFT, BOX_TOP, BOX_RIGHT, BOX_BOTTOM = [input('Define AOI West boundary?: '),\n",
    "                                                input('Define AOI North boundary?: '),\n",
    "                                                input('Define AOI East boundary?: '),\n",
    "                                                input('Define AOI South boundary?: ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362346ce-05f4-44d4-80d7-3c336475f80b",
   "metadata": {},
   "source": [
    "### SETUP THE DATA DIRECTORY STRUCTURE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf2483-9d19-4880-b706-73f40e7b630b",
   "metadata": {},
   "source": [
    "##### DATA DIRECTORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bba1a-2db6-44db-963f-f21cf6d451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = MAIN_DIR / Path('data_repo/data/nisar/' + SITE + '/')\n",
    "os.makedirs(DATA_DIR, exist_ok='True') # CREATES DATA DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad787e-9a88-44cf-a525-831aa1747e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Directory where you input LIDAR AGB Map of ROI should be stored for processing \n",
    "LIDAR_IN_DIR = MAIN_DIR / Path('data_repo/data/LiDAR/' + SITE + '/')\n",
    "os.makedirs(LIDAR_IN_DIR, exist_ok='True') # CREATES RESAMPLED LiDAR MAP DIRECTORY IF NOT EXISTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795462c-6c33-48af-97e3-e611e28fcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Directory where you input LULC Map of ROI should be stored for processing \n",
    "LULC_IN_DIR = MAIN_DIR / Path('data_repo/data/LULC/' + SITE + '/')\n",
    "os.makedirs(LULC_IN_DIR, exist_ok='True') # CREATES RESAMPLED LiDAR MAP DIRECTORY IF NOT EXISTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47d8da-5567-4a0d-b9aa-3154f066fa53",
   "metadata": {},
   "source": [
    "##### PROCESSING DIRECTORIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0c96f-8ac3-45de-bdb2-75ab2d8cba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PROCESSING DIRECTORY \n",
    "PROCESS_DIR = MAIN_DIR / Path('data_repo/processing/nisar/') /  SITE \n",
    "os.makedirs(PROCESS_DIR, exist_ok='True') # CREATES Processing DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f3fb6-cb2a-42ab-8c4c-ff7becdd9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### META DATA DIRECOTRY \n",
    "META_DIR = PROCESS_DIR / 'Metadata'\n",
    "os.makedirs(META_DIR, exist_ok='True') # CREATES META DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79552be8-fd78-4f6d-89eb-abadbbbddc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAR GCOV TIMESERIES GEOTIFF \n",
    "GCOV_DIR = PROCESS_DIR / 'GCOV/'\n",
    "os.makedirs(GCOV_DIR, exist_ok='True') # CREATES SAR TIMESERIES DIRECTORY IF NOT EXISTING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82703a0c-77bc-408d-856c-5cd01ad75d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESAMPLED SAR TIMESERIES GEOTIFF \n",
    "GCOV_STACKS_DIR = PROCESS_DIR / 'GCOV_STACKS/'\n",
    "os.makedirs(GCOV_STACKS_DIR, exist_ok='True') # CREATES RESAMPLED SAR TIMESERIES DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14e771-2f65-4636-8ad5-e057beeb880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESAMPLED LiDAR MAPS\n",
    "LIDAR_DIR = PROCESS_DIR / 'LiDAR'\n",
    "os.makedirs(LIDAR_DIR, exist_ok='True') # CREATES RESAMPLED LiDAR MAP DIRECTORY IF NOT EXISTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015d671-0652-42e2-942d-19a652df9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESAMPLED LULC MAPS\n",
    "LULC_DIR = PROCESS_DIR / 'LULC'\n",
    "os.makedirs(LULC_DIR, exist_ok='True') # CREATES RESAMPLED LULC DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5313bec-7dc6-4788-a98a-fc9c9120b603",
   "metadata": {},
   "source": [
    "##### OUTPUT DIRECTORIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dacc85-52e1-4322-a6f9-c5257fe94a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUTPUT DIRECTORY \n",
    "OUTDIR_DIR = MAIN_DIR / Path('data_repo/output/nisar/') /  SITE \n",
    "os.makedirs(OUTDIR_DIR, exist_ok='True')  # CREATES OUTPUT DIRECTORY IF NOT EXISTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e9345-3f38-48af-a9c4-301a5378c8e9",
   "metadata": {},
   "source": [
    "### 1. Data Downloading and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f195367-505c-453c-8ddf-ad3c2ba51de9",
   "metadata": {},
   "source": [
    "This notebook is used to set up the directories, download NISAR GCOV Dataset, and perform exploratory data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11457493-93b3-4234-ba88-ecd070dbc1fb",
   "metadata": {},
   "source": [
    "##### Get the credentials for ASF/Earthaccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11466e2-d02c-40da-bfff-fb2ca26f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the credentials and parameters\n",
    "DATABASE_NAME = input('Select The Database ? (Options: DEMO or earthaccess or asf_search): ')\n",
    "\n",
    "if DATABASE_NAME == 'DEMO':\n",
    "    print('Enter the DEMO database')\n",
    "    \n",
    "elif DATABASE_NAME == 'earthaccess':\n",
    "    os.environ[\"EARTHDATA_USERNAME\"] = input(\"Enter your Earthdata username: \")\n",
    "    os.environ[\"EARTHDATA_PASSWORD\"] = getpass.getpass(\"Enter your Earthdata password: \")\n",
    "    \n",
    "elif DATABASE_NAME == 'asf_search':\n",
    "    os.environ[\"ASF_USERNAME\"] = input(\"Enter your ASF username: \")\n",
    "    os.environ[\"ASF_PASSWORD\"] = getpass.getpass(\"Enter your ASF password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af192f7-db2f-4b5f-9bf0-8f99e59b52d8",
   "metadata": {},
   "source": [
    "##### Query and Stream the DEMO/ASF/Earthaccess Database for Sample/NISAR dataset and convert H5 Files to GEOTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18052d24-ed6e-413f-9b54-bbf49cff09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_NAME == 'DEMO':\n",
    "    gcov_url = ('s3://nisar-public-ebd/ATBD/ecosystems/disturbance/sites/leno/GCOV/simulated/*')\n",
    "    s3 = s3fs.S3FileSystem(anon=True,endpoint_url='https://s3.us-west-1.wasabisys.com')\n",
    "    ALL_GCOV_DATA = ['s3://' + k  for k in s3.glob(gcov_url)]\n",
    "    print(\"\\n\".join([path for path in ALL_GCOV_DATA]))\n",
    "\n",
    "    \n",
    "elif DATABASE_NAME == 'earthaccess':    \n",
    "    AUTH = earthaccess.login(strategy=\"environment\")\n",
    "    s3cred = AUTH.get_s3_credentials(endpoint=\"https://nisar.asf.earthdatacloud.nasa.gov/s3credentials\")\n",
    "    \n",
    "    RESULTS = earthaccess.search_data(short_name = 'NISAR_L2_GCOV_BETA_V1', \n",
    "                                        bounding_box = (BOX_LEFT, BOX_BOTTOM, BOX_RIGHT, BOX_TOP)\n",
    "                                       )\n",
    "    ## Get S3 URLS for search results\n",
    "    ALL_GCOV_DATA = [earthaccess.results.DataGranule.data_links(x, access='direct')[0] for x in RESULTS if earthaccess.results.DataGranule.data_links(x, access='direct')[0].endswith('.h5')]\n",
    "    ALL_GCOV_DATA = sorted(ALL_GCOV_DATA)\n",
    "    print(\"\\n\".join([path for path in ALL_GCOV_DATA]))\n",
    "\n",
    "    \n",
    "    s3 = s3fs.S3FileSystem(anon = False, key=s3cred['accessKeyId'], \n",
    "                           secret = s3cred['secretAccessKey'],\n",
    "                           token = s3cred['sessionToken'],\n",
    "                           client_kwargs ={'region_name': 'us-west-2'})\n",
    "    \n",
    "\n",
    "elif DATABASE_NAME == 'asf_search':\n",
    "    SESSION = asf.ASFSession()\n",
    "    SESSION.auth_with_creds(os.environ.get(\"ASF_USERNAME\"), os.environ.get(\"ASF_PASSWORD\"))\n",
    "    \n",
    "    WKT = f'POLYGON(({BOX_RIGHT} {BOX_BOTTOM}, {BOX_LEFT} {BOX_BOTTOM}, {BOX_LEFT} {BOX_TOP}, {BOX_RIGHT} {BOX_TOP}, {BOX_RIGHT} {BOX_BOTTOM}))'\n",
    "    \n",
    "    RESULTS = asf.search(\n",
    "        platform=['NISAR'],\n",
    "        shortName='NISAR_L2_GCOV_BETA_V1', # Replace with the desired product short name\n",
    "        intersectsWith=WKT,\n",
    "        #start='2025-01-01T00:00:00Z',\n",
    "        #end='2025-01-31T23:59:59Z',\n",
    "        maxResults=10 # Limit results for example\n",
    "        )\n",
    "\n",
    "    ALL_GCOV_DATA = [x['properties']['s3Urls'][y] for x in RESULTS.geojson()['features'] for y in range(len(x['properties']['s3Urls'])) if x['properties']['s3Urls'][y].endswith('.h5') and 'STATS' not in x['properties']['s3Urls'][y]]\n",
    "    ALL_GCOV_DATA = sorted(ALL_GCOV_DATA)\n",
    "    print(\"\\n\".join([path for path in ALL_GCOV_DATA]))\n",
    "    \n",
    "    response = SESSION.get(\"https://nisar.asf.earthdatacloud.nasa.gov/s3credentials\")\n",
    "    s3cred = response.json()\n",
    "    s3 = s3fs.S3FileSystem(key=s3cred['accessKeyId'], \n",
    "                           secret = s3cred['secretAccessKey'],\n",
    "                           token = s3cred['sessionToken'],\n",
    "                           client_kwargs ={'region_name': 'us-west-2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51de3c2-7b27-421e-8dfd-ac9feb638caa",
   "metadata": {},
   "source": [
    "#### Get the Dates of the NISAR /NISAR Simulated GCOV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257e2ff-9f69-427f-8253-2462c77a985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CREATE A METAFILE WITH LIST OF ALL PRODUCTS AND DATES AND SAVE\n",
    "DATES = [fn.date_from_filename(filename) for filename in  ALL_GCOV_DATA]  # GET THE DATE OF DATA ACQUISITION FOR EACH PRODUCT ID\n",
    "DATES = sorted(DATES) # SORT THE DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f355e5e-3ab9-469b-9c2b-7916ce81640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT THE PRODUCT ID BASED ON DATES\n",
    "ALL_GCOV_DATA = sorted(ALL_GCOV_DATA, key=fn.date_from_filename)\n",
    "ALL_GCOV_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cde78-46fc-4608-a90d-75e66e66dbbc",
   "metadata": {},
   "source": [
    "#### Save the List of NISAR GCOV Data to the CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf62160-9c5a-4bd0-8a15-255a6cabbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A LIST WITH DATES AND PRODUCT ID\n",
    "SAR_DATA_ID = pd.DataFrame()\n",
    "for BAND_INDEX, path in enumerate(ALL_GCOV_DATA):\n",
    "    dict_ = {'Dates':DATES[BAND_INDEX],'FILENAME': os.path.basename(path).replace('.h5', ''), 'POL': os.path.basename(path)[36:38], 'PATH': path}\n",
    "    df = pd.DataFrame([dict_])\n",
    "    SAR_DATA_ID = pd.concat([SAR_DATA_ID, df], ignore_index=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "SAR_DATA_ID.style.set_properties(**{'text-align': 'center'})\n",
    "display(SAR_DATA_ID)\n",
    "\n",
    "# SAVE THE DATAFRAME TO META DIRECTORY \n",
    "SAR_DATA_ID.to_csv(META_DIR / Path(SITE + '_NISAR_Data_List.csv'), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8489461-16a4-46d3-9933-414fb82e3c5d",
   "metadata": {},
   "source": [
    "#### Convert the Streamed NISAR Simulated H5 Files to GEOTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d7d21-3ab4-46a8-b4a9-0f0d4305080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_DATA_LIST = []\n",
    "for SAR_ID in range(0, len(SAR_DATA_ID)):\n",
    "    FILE = SAR_DATA_ID.iloc[SAR_ID]['PATH'] \n",
    "    if DATABASE_NAME == 'DEMO':\n",
    "        F = h5py.File(s3.open(FILE, \"rb\"))\n",
    "    elif DATABASE_NAME == 'earthaccess':\n",
    "        F = h5py.File(FILE,\n",
    "                  driver='ros3', aws_region=b'us-west-2' , \n",
    "                  session_token = s3cred['sessionToken'].encode('utf-8'),\n",
    "                  secret_id=s3cred['accessKeyId'].encode('utf-8'), \n",
    "                  secret_key=s3cred['secretAccessKey'].encode('utf-8'), mode=\"r\",\n",
    "                  page_buf_size = 2 * 1024**3) \n",
    "\n",
    "        \n",
    "    elif DATABASE_NAME == 'asf_search':\n",
    "        F = h5py.File(\n",
    "                    FILE,\n",
    "                    driver='ros3',\n",
    "                    aws_region=b'us-west-2',\n",
    "                    secret_id=s3cred['accessKeyId'].encode('utf-8'),      # Changed from aws_access_key_id\n",
    "                    secret_key=s3cred['secretAccessKey'].encode('utf-8'),  # Changed from aws_secret_access_key\n",
    "                    session_token=s3cred['sessionToken'].encode('utf-8'),\n",
    "                    mode=\"r\"\n",
    "                    )\n",
    "    \n",
    "    FILE_ID = os.path.basename(FILE)\n",
    "    A_GROUP_KEY = list(F.keys())[0]\n",
    "    DS_X = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()]      # returns as a h5py dataset object\n",
    "    DS_Y = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()]      # returns as a h5py dataset object\n",
    "    DS_EPSG = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['projection'][()]\n",
    "    DS_HHHH = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['HHHH'][()]  # returns as a numpy array\n",
    "    DS_HVHV = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['HVHV'][()]  # returns as a numpy array\n",
    "    DS_MASK = F[A_GROUP_KEY]['LSAR']['GCOV']['grids']['frequencyA']['mask'][()]  # returns as a numpy array\n",
    "    \n",
    "    print('Raster bounds: ',min(DS_X),max(DS_X),min(DS_Y),max(DS_Y))\n",
    "    print('X Size: ',DS_X.shape[0],' Y Size: ',DS_Y.shape[0])\n",
    "    print('Resolution: ', DS_Y[0] - DS_Y[1],'m')\n",
    "    print('')\n",
    "    meta = {'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': None, \n",
    "            'width': DS_X.shape[0], \n",
    "            'height': DS_Y.shape[0], \n",
    "            'count': 1, \n",
    "            'crs': rasterio.CRS.from_epsg(DS_EPSG[()]), \n",
    "            'transform': rasterio.Affine(DS_X[1] - DS_X[0], 0.0, DS_X[0], 0.0, DS_Y[1] - DS_Y[0], DS_Y[0]), \n",
    "            'tiled': False, \n",
    "            'interleave': 'band'}\n",
    "\n",
    "    if DATABASE_NAME != 'DEMO':\n",
    "        MASK_NAN =  DS_MASK == 1\n",
    "        DS_HHHH[MASK_NAN==0] = np.nan\n",
    "        DS_HVHV[MASK_NAN==0] = np.nan\n",
    "\n",
    "    with rasterio.open(GCOV_DIR / FILE_ID.replace('.h5', '_HHHH_power.tif'), 'w', **meta) as dst:\n",
    "        dst.write(DS_HHHH,indexes=1)\n",
    "\n",
    "    with rasterio.open(GCOV_DIR / FILE_ID.replace('.h5', '_HVHV_power.tif'), 'w', **meta) as dst:\n",
    "            dst.write(DS_HVHV,indexes=1) \n",
    "\n",
    "       \n",
    "    SAR_DATA_LIST.append(GCOV_DIR / FILE_ID.replace('.h5', '_HVHV_power.tif'))\n",
    "    SAR_DATA_LIST.append(GCOV_DIR / FILE_ID.replace('.h5', '_HHHH_power.tif'))\n",
    "    \n",
    "    print(FILE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c18d1e-e79e-4858-af8d-a3f94ca9c6a3",
   "metadata": {},
   "source": [
    "#### Create a reference file of 100 m X100 m resolution  \n",
    "To create L3 product at 100 m resolution, the reference image is generated to resample all products to 100 m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806c7f2-335f-4772-80ce-e293fc74e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE NISAR PRODUCTS ARE VALIDATED AT 1-ha (100 m X100 m) RESOLUTION\n",
    "INPUT_FILE     = SAR_DATA_LIST[0] # PATH TO THE INPUT FILE FOR CREATING REFERENCE IMAGE\n",
    "REFERENCE_FILE = GCOV_STACKS_DIR / Path(SITE + \"_100m_REF.tif\") # PATH OF THE CREATED REFERENCE IMAGE  \n",
    "print(INPUT_FILE.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d2c2a-4735-416b-907b-fb9fa4dc9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A GDAL EXPRESSION TO RESAMPLE THE INPUT FILE BY AVERAGING OVER 100 m X100 m  \n",
    "gdal_expression = f'gdal_translate -tr 100 100 -r average {INPUT_FILE} {REFERENCE_FILE}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ab1a3-854b-49a5-888c-53923cbea161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE GDAL EXPRESSSION THROUGH THE SHELL AND CAPTURE ITS OUTPUT\n",
    "subprocess.check_output(gdal_expression, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab858e7-1417-43bf-a9bb-d5a805cc6104",
   "metadata": {},
   "source": [
    "#### Resample all RTC SAR images to reference image geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51360d-9f40-4887-90b4-01493eab5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESAMPLE ALL RTC SAR DATA TO REFERENCE IMAGE AND STORE INTO \n",
    "SAR_RES_DATA_LIST = []\n",
    "for IN_FILE in SAR_DATA_LIST:\n",
    "    # RESAMPLED OUTPUT FILE NAME \n",
    "    OUT_FILE = GCOV_STACKS_DIR / os.path.basename(IN_FILE).replace(\".tif\", \"_100m.tif\")\n",
    "    # RESAMPLED INPUT FILE TO REFERENCE GEOMETRY \n",
    "    fn.raster_clip(REFERENCE_FILE, IN_FILE, OUT_FILE, resampling_method=\"average\")  \n",
    "    # APPEND THE RESAMPLED FILE NAME\n",
    "    SAR_RES_DATA_LIST.append(OUT_FILE) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b121e5f-521c-40b4-a311-e0fab3faa076",
   "metadata": {},
   "source": [
    "#### Save the List of Resampled NISAR Simulated ALOS2 Data to the CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce05f9-2de1-4dc3-8e4f-83cdbcb9a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_RES_DATA_ID = pd.DataFrame(data = SAR_RES_DATA_LIST, columns =['DATA_ID'])  # CONVERT THE RESAMPLED LIST TO DATAFRAME \n",
    "SAR_RES_DATA_ID.to_csv(META_DIR / Path(SITE + '_NISAR_Band_List.csv'), index=False) # SAVE THE DATAFRAME TO META DIRECTORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20bbe4-25bd-4336-b12c-62d8eebe16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "display(SAR_RES_DATA_ID) # DISPLAY THE BANDS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82003a5-6780-4e69-a422-b42bdf51c27c",
   "metadata": {},
   "source": [
    "### Image Statistics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb00b0f-4140-4e10-9912-ff531cd7d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CALCULATE IMAGE STATISTICS \n",
    "NUM_SCENES = int(len(SAR_RES_DATA_LIST)/2)\n",
    "# INITIALIZE THE HH FILE LIST\n",
    "HH_FILES = []  \n",
    "# INITIALIZE THE HV FILE LIST\n",
    "HV_FILES = []  \n",
    "for NUM in range(NUM_SCENES):\n",
    "    # HH data \n",
    "    HV_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 0]) # APPEND THE HV FILES \n",
    "    # HV data \n",
    "    HH_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 1]) # APPEND THE HH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9da02b-c487-4c09-b88b-64e66956accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523484b6-86b4-4735-803a-22061638aa52",
   "metadata": {},
   "source": [
    "### HV Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcaa348-51e5-4ee2-b4e6-e56d1cba7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE HV IMAGE STATISTICS \n",
    "DF_HV = fn.image_statistics(HV_FILES, SITE, POL = 'HV') \n",
    "# WRITE THE DATAFRAME OF HV IMAGE STATISTICS TO CSV FILE \n",
    "DF_HV.to_csv(META_DIR / Path(SITE + \"_HV_STATS.csv\")) \n",
    "# DISPLAY THE HV STATISTICS\n",
    "display(DF_HV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49995abc-f797-4dbf-a315-3dd7fb6b3fb3",
   "metadata": {},
   "source": [
    "### HH Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1ba2d-d56b-4e94-b070-fd3329605419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE HH IMAGE STATISTICS\n",
    "DF_HH = fn.image_statistics(HH_FILES, SITE, POL = 'HH')   \n",
    "# WRITE THE DATAFRAME OF HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HH.to_csv(META_DIR / Path(SITE + \"_HH_STATS.csv\")) \n",
    "# DISPLAY THE HH STATISTICS\n",
    "display(DF_HH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ebcbc-5120-49b8-a16a-713d1042e150",
   "metadata": {},
   "source": [
    "### SAR Time Series Visualization and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbaeac5-d609-49c9-ac77-f899fe7ffa70",
   "metadata": {},
   "source": [
    "#### Plot NISAR Simulated ALOS2 HH and HV Polarization Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a5fc4-62e8-46df-9a83-6ea9d5f353cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PLOT EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES): #\n",
    "    plt.rcParams['font.family'] = 'serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "    plt.rcParams['font.serif'] = ['DejaVu Serif']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "    plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "    # CREATES A FIGURE AND A GRID FOR SUBPLOTS \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 11)) \n",
    "    \n",
    "    # LOAD THE HH IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HH = 10 * np.log10(HH_INFO.GetRasterBand(1).ReadAsArray()) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HV = 10 * np.log10(HV_INFO.GetRasterBand(1).ReadAsArray()) # READ THE HH DATA INTO ARRAY  \n",
    "    \n",
    "    # GET THE EXTENT OF THE IMAGE  \n",
    "    X_MIN, X_MAX, Y_MIN, Y_MAX = fn.GetExtent(HH_INFO)   \n",
    "    # GET THE DATE OF THE IMAGE IN DD-MM-YYYY FORMAT\n",
    "    DATE = SAR_DATA_ID.Dates.iloc[NUM].strftime(\"%d-%m-%Y\") \n",
    "\n",
    "    if DEMO:\n",
    "        vmin = -20\n",
    "        vmax = 0\n",
    "    else:\n",
    "        vmin = -25\n",
    "        vmax = 0\n",
    "    \n",
    "    # DISPLAYS THE HH IMAGE \n",
    "    im = ax[0].imshow(\n",
    "        GAMMA_HH,\n",
    "        cmap = \"gray\",\n",
    "        vmin = vmin,\n",
    "        vmax = vmax,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "    )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax[0].grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[0].set_title(\"HH Backscatter Image: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[0].set_xlabel('Easting (m)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[0].set_ylabel('Northing (m)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax[0].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax[0].yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADDS A COLORBAR \n",
    "    cbar = ax[0].figure.colorbar(im, ax=ax[0], shrink=0.6, orientation='horizontal', pad=0.1)\n",
    "    # CUSTOMIZING THE COLOR BAR LABEL \n",
    "    cbar.set_label('$dB$', labelpad=-35, x = 1.05, y = 1.1, rotation=0)\n",
    "\n",
    "    if DEMO:\n",
    "        vmin = -25\n",
    "        vmax = -5\n",
    "    else:\n",
    "        vmin = -30\n",
    "        vmax = -5\n",
    "\n",
    "\n",
    "    \n",
    "    # DISPLAYS THE HV IMAGE \n",
    "    im = ax[1].imshow(\n",
    "        GAMMA_HV,\n",
    "        cmap = \"gray\",\n",
    "        vmin = vmin,\n",
    "        vmax = vmax,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "    )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax[1].grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[1].set_title(\"HV Backscatter Image: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[1].set_xlabel('Longitude (Deg.)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[1].set_ylabel('Lattitude (Deg.)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax[1].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax[1].yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADDS A COLORBAR \n",
    "    cbar = ax[1].figure.colorbar(im, ax=ax[1], shrink=0.6, orientation='horizontal', pad=0.1)\n",
    "    # CUSTOMIZING THE COLOR BAR LABEL \n",
    "    cbar.set_label('$dB$', labelpad=-35, x = 1.05, y = 1.1, rotation=0)\n",
    "    \n",
    "    # ADJUST HORIZONTAL AND VERTICAL SPACING BETWEEN SUBPLOTS \n",
    "    plt.subplots_adjust(wspace=0.35, hspace=0.25)\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(META_DIR / Path(SITE + \"_SAR_Backscatter_\" + DATE + \".png\"), dpi=600, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6097bf2-c8fa-4718-a69b-ae406be463ba",
   "metadata": {},
   "source": [
    "#### PLOT STATISTIC OF EACH SAR IMAGES FROM THE FILTERED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaec972-a2dc-4164-af8a-dce4d3af96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT STATISTICS OF EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES):\n",
    "    plt.rcParams['font.family'] = 'serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "    plt.rcParams['font.serif'] = ['DejaVu Serif']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "    plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "    # CREATES A FIGURE AND A GRID FOR SUBPLOTS \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6)) \n",
    "\n",
    "    # LOAD THE HH IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    DATA = HH_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    MASK = (DATA >  0.0001)  &  (~ np.isnan(DATA)) & (~ np.isinf(DATA))\n",
    "    DATA_VAL = DATA[MASK == 1] # EXTRACT THE DATA BASED ON MASK\n",
    "    GAMMA_HH = 10*np.log10(DATA_VAL)\n",
    "\n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    DATA = HV_INFO.GetRasterBand(1).ReadAsArray()\n",
    "    MASK = (DATA >  0.0001)  &  (~ np.isnan(DATA)) & (~ np.isinf(DATA))\n",
    "    DATA_VAL = DATA[MASK == 1] # EXTRACT THE DATA BASED ON MASK\n",
    "    GAMMA_HV = 10*np.log10(DATA_VAL)\n",
    "    \n",
    "    # # GET THE DATE OF THE IMAGE IN DD-MM-YYYY FORMAT\n",
    "    DATE = SAR_DATA_ID.Dates.iloc[NUM].strftime(\"%d-%m-%Y\")  \n",
    "    \n",
    "    #  CREATES A HISTOGRAM FOR HH SAR IMAGE \n",
    "    im = ax[0].hist(GAMMA_HH.flatten(), bins= np.arange(-30, 10, 0.2),  color = 'dodgerblue', alpha = 0.5)\n",
    "    #  PLOTS THE MEAN VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN']),   color='blue',    linestyle='-', linewidth=2, label = 'Mean')\n",
    "    #  PLOTS THE MEDIAN VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEDIAN']), color='red', linestyle='--', linewidth=2, label = 'Median')\n",
    "    # PLOTS THE MEAN + SD VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN'] - DF_HH.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2, label = 'Mean ± SD')\n",
    "    #  PLOTS THE MEAN - SD VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(10*np.log10(DF_HH.iloc[NUM]['MEAN'] + DF_HH.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2)\n",
    "    #  PLOTS THE 1 PERCENTILE  VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(np.nanpercentile(GAMMA_HH, 5),  color = 'saddlebrown', linestyle='--',  linewidth=2, label = '5 Percentile')\n",
    "    #  PLOTS THE 99 PERCENTILE  VALUE OF HH SAR IMAGE \n",
    "    ax[0].axvline(np.nanpercentile(GAMMA_HH, 95), color = 'gold', linestyle='--',  linewidth=2, label = '95 Percentile')\n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[0].set_title(\"HH Backscatter Image Statistics: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[0].set_xlabel('HH Backscattered power (dB)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[0].set_ylabel('Count',  fontweight = 'bold')\n",
    "    # SET THE XLIM \n",
    "    #ax[0].set_xlim([-10, 0])\n",
    "    # ADD THE LEGEND  \n",
    "    ax[0].legend()\n",
    "    \n",
    "    \n",
    "    #  CREATES A HISTOGRAM FOR HV SAR IMAGE \n",
    "    im = ax[1].hist(GAMMA_HV.flatten(), bins= np.arange(-40, 5, 0.2), color = 'dodgerblue', alpha = 0.5)\n",
    "    #  PLOTS THE MEAN VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN']),   color='blue',    linestyle='-', linewidth=2, label = 'Mean')\n",
    "    #  PLOTS THE MEDIAN VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEDIAN']), color='red', linestyle='--', linewidth=2, label = 'Median')\n",
    "    #  PLOTS THE MEAN + SD VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN'] - DF_HV.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2, label = 'Mean ± SD')\n",
    "    #  PLOTS THE MEAN - SD VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(10*np.log10(DF_HV.iloc[NUM]['MEAN'] + DF_HV.iloc[NUM]['SD']), color='k', linestyle='--', linewidth=2)\n",
    "    #  PLOTS THE 1 PERCENTILE  VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(np.nanpercentile(GAMMA_HV, 5),  color = 'saddlebrown', linestyle='--',  linewidth=2, label = '5 Percentile')\n",
    "    #  PLOTS THE 99 PERCENTILE  VALUE OF HV SAR IMAGE \n",
    "    ax[1].axvline(np.nanpercentile(GAMMA_HV, 95), color = 'gold', linestyle='--',  linewidth=2, label = '95 Percentile')\n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax[1].set_title(\"HV Backscatter Image Statistics: \" + DATE, fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax[1].set_xlabel('HV Backscattered power (dB)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax[1].set_ylabel('Count',  fontweight = 'bold')\n",
    "    # SET THE XLIM \n",
    "    #ax[1].set_xlim([-20, -5])\n",
    "    # ADD THE LEGEND  \n",
    "    ax[1].legend()\n",
    "    \n",
    "    # ADJUST HORIZONTAL AND VERTICAL SPACING BETWEEN SUBPLOTS \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.25)\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(META_DIR / Path(SITE  + \"_SAR_Backscatter_Statistics_\" + DATE + \".png\"), dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144292f-8b90-4c4e-b26d-bdb8db32e893",
   "metadata": {},
   "source": [
    "##### FILTER THE DATA, CREATE METADATA FILE, VERSION NUMBER AND FILE NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77cd69-cb96-4731-ae79-6ef6d93338a3",
   "metadata": {},
   "source": [
    "#### Data Filtering: Specifying Exclusions\n",
    "To refine the dataset entering the processing chain, this function allows you to specify data records for exclusion. Please provide the corresponding row numbers for any data you wish to omit from the operation.\n",
    "> **Input Format Specifications**<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  The system accepts two syntax formats for defining the excluded rows:<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a). *Comma-Separated Values:* For non-contiguous rows, enter each individual row number separated by a comma.<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example: 3, 8, 14 (This will exclude rows 3, 8, and 14).<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b). *Hyphenated Ranges:* For a continuous block of rows, enter the starting row number followed by a hyphen and the ending row number.<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example: 3-5 (This will exclude rows 3, 4, and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432e49-18ba-4294-9634-861e24613324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILTER THE SAR DATASET \n",
    "DF = pd.read_csv(META_DIR / Path(SITE + '_NISAR_Data_List.csv'))\n",
    "\n",
    "# REMOVE THE DATES YOU WANT TO EXCLUDE FROM THE PROCESSING \n",
    "DF_FILTERED = fn.filter_data(DF)\n",
    "\n",
    "# VIEW THE FILTERED DATALIST \n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Disable column width truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Optional: set display width to unlimited (so it doesn't wrap lines)\n",
    "pd.set_option('display.width', None)\n",
    "display(DF_FILTERED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd32f1a-41bb-4eb1-a045-2895a0841b62",
   "metadata": {},
   "source": [
    "##### Create a version number and filename of the current product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0145101-c799-4029-aa86-f2beb848f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE VERSION NUMBER OF FILE IF PREVIOUS RUN EXISTS\n",
    "# Change OVERWRITE to false for newer version\n",
    "\n",
    "V_N = str(fn.get_version_number(str(META_DIR), OVERWRITE = True)).zfill(3)\n",
    "\n",
    "### CREATE THE NEW FILE NAME FOR THE CURRENT RUN BASED ON NAMING CONVENTION\n",
    "# NISAR_B_L_BW_F_PROD_SM_EM_YY_SITE_VVV_PRODDATE.EXT\n",
    "# B - Frequency Band (L or S or B for L & S)\n",
    "# L - Product Level (3)\n",
    "# BW - Bandwidth (20 Mhz, 40 Mhz)\n",
    "# F - Flight path (A, D, B)\n",
    "# PROD - Product (AGB)\n",
    "# SM - Start Month\n",
    "# EM - End Month\n",
    "# YY - Year\n",
    "# SITE - Cal/Val Site name\n",
    "# VVV version number of the product\n",
    "# PRODDATE - time of processing \n",
    "\n",
    "### CREATE THE NEW FILE NAME FOR THE CURRENT RUN\n",
    "FILE_NAME  = 'NISAR_L_3_20_B_AGB_'  + f\"{datetime.strptime(DF_FILTERED['Dates'].iloc[0], \"%Y-%m-%d\").date().month:02}\" + '_' + f\"{datetime.strptime(DF_FILTERED['Dates'].iloc[-1], \"%Y-%m-%d\").date().month:02}\" + '_' + f\"{datetime.strptime(DF_FILTERED['Dates'].iloc[-1], \"%Y-%m-%d\").date().year % 100:02}\" + '_' + SITE + '_' + f\"{V_N:03}\" + '_' + str(datetime.now().strftime(\"%Y%m%d\")) \n",
    "\n",
    "# SAVE THE LIST OF FILTERED SAR DATA LIST TO CSV\n",
    "DF_FILTERED.to_csv(META_DIR / Path(FILE_NAME + '_FILTERED_DATA.csv'), index=False)\n",
    "\n",
    "print('The Filename is ' + FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a60d3-1ebf-4cd7-8bf7-65e3f52ded59",
   "metadata": {},
   "source": [
    "##### Create a output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628555d3-34aa-41d0-b864-f4a2ca2a3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATES OUTPUT DIRECTORY IF NOT EXISTING \n",
    "OUT_DIR = OUTDIR_DIR / FILE_NAME  \n",
    "os.makedirs(OUT_DIR, exist_ok='True')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7b4ec-4670-4ced-9b81-397cb83f1eec",
   "metadata": {},
   "source": [
    "##### Filtered SAR data datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1518eac-99f4-47c8-b619-8a4bf9222e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AND READ THE CSV FILE AND GET THE FILE NAMES\n",
    "SAR_RES_DATA_LIST = [os.path.basename(x) for x in  list(pd.read_csv(META_DIR / Path(SITE + '_NISAR_Band_List.csv'))['DATA_ID'])]\n",
    "# LOAD THE FILE NAMES TO RETRIEVE  \n",
    "DF_FILTERED_LIST = [os.path.basename(x).split('.h5')[0] for x in DF_FILTERED['FILENAME'].tolist()] \n",
    "#GET THE PATH OF THE DATA TO BE RETAINED\n",
    "SAR_RES_DATA_LIST = [str(GCOV_STACKS_DIR) + '/' + s for s in SAR_RES_DATA_LIST if any(sub in s for sub in DF_FILTERED_LIST)]\n",
    "print(\"\\n\".join([os.path.basename(path) for path in DF_FILTERED_LIST]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14c649-7b01-4dd5-8170-8b5a1db58239",
   "metadata": {},
   "source": [
    "##### Filtered HV Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613637d1-47a0-4e78-a240-7c7f8f57e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_HV_FILTERED = DF_HV[DF_HV['BAND_NAME'].isin(DF_FILTERED_LIST)]\n",
    "# WRITE THE DATAFRAME OF FILTERED HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HV_FILTERED.to_csv(OUT_DIR / Path(FILE_NAME + \"_HV_STATS.csv\")) \n",
    "# VIEW THE FILTERED DATALIST \n",
    "display(DF_HV_FILTERED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0149b1-0ec6-4fe8-a377-e0c682895e52",
   "metadata": {},
   "source": [
    "##### Filtered HH Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb56289-4b56-4097-b5b8-fdf4f9d22149",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_HH_FILTERED = DF_HH[DF_HH['BAND_NAME'].isin(DF_FILTERED_LIST)]\n",
    "# WRITE THE DATAFRAME OF FILTERED HH IMAGE STATISTICS TO CSV FILE\n",
    "DF_HH_FILTERED.to_csv(OUT_DIR / Path(FILE_NAME + \"_HH_STATS.csv\")) \n",
    "# VIEW THE FILTERED DATALIST \n",
    "display(DF_HH_FILTERED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c418a2-8132-474b-943d-e78432900cbf",
   "metadata": {},
   "source": [
    "### 2. Create Calibration / Validation Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27625140-e5a5-4dda-997b-3d65a4d72afe",
   "metadata": {},
   "source": [
    "In this section, we generate two distinct mask layers:\n",
    "1. **Scene Mask:** Covers the entire NISAR scene using valid HH and HV observations and Land Use/Land Cover (LULC) classes.\n",
    "2. **Cal/Val Mask:** Covers the calibration/validation area, incorporating valid Aboveground Biomass (AGB) pixels ($>0$ Mg/ha) along with the Scene mask.\n",
    "\n",
    "*Workflow:*\n",
    "\n",
    "1. **Temporal Statistics:** Compute the minimum ($HH_{min}$, $HV_{min}$) and maximum ($HH_{max}$, $HV_{max}$) backscatter values for every pixel in the image over the entire time series.\n",
    "2. **Backscatter Mask:** Select pixels where $HV_{min}$ exceeds the defined threshold, and both $HH_{min}$ and $HV_{min}$ are less than 1.\n",
    "3. **LULC Mask:** Filter the Landcover data to create a mask that excludes water, man-made structures, and non-vegetation classes.\n",
    "4.  **Forest/Non-Forest (FNF) Scene Mask:** Combine the Backscatter and LULC masks to generate the final mask for the entire NISAR scene.\n",
    "5. **AGB Mask (Cal/Val Only):** Within the calibration/validation area, utilize the LiDAR AGB Map to exclude non-AGB values (where AGB $\\leq 0$).\n",
    "6. **Final Cal/Val Mask:** Combine the Backscatter, LULC, and AGB masks to generate the FNF mask over the Cal/Val area.\n",
    "\n",
    "> **Note:** For this demonstration, the model calibration parameters are pre-defined. We will proceed with generating the mask for the entire SAR scene. However, the steps above outline how to generate the specific Cal/Val mask if the requisite AGB map is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96658bbb-bc12-4ed3-917d-2eb220408a55",
   "metadata": {},
   "source": [
    "##### Temporal Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256b32a-bf32-4211-af02-733ded053fbf",
   "metadata": {},
   "source": [
    "##### Create Minimum and Maximum Backscatter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d403c3-c803-40b6-a480-c95eda7b0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH_FILES = []  # INITIALIZE THE HH FILE LIST\n",
    "HV_FILES = []  # INITIALIZE THE HV FILE LIST\n",
    "# CALCULATE NUMBER OF SCENES\n",
    "NUM_SCENES = int(len(SAR_RES_DATA_LIST)/2)\n",
    "\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # HH data \n",
    "    HV_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 0]) # APPEND THE HV FILES \n",
    "    # HV data \n",
    "    HH_FILES.append(SAR_RES_DATA_LIST[NUM*2 + 1]) # APPEND THE HH FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb466f-e957-42bb-93d7-116ab8a1f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE HV MIN AND MAX LAYER \n",
    "DA_HV = []\n",
    "# PLOT STATISTICS OF EACH SAR IMAGES FROM THE FILTERED DATASET\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # LOAD THE HV IMAGE\n",
    "    HV_INFO = gdal.Open(HV_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HV = HV_INFO.GetRasterBand(1).ReadAsArray() # READ THE HH DATA INTO ARRAY  \n",
    "    DA_HV.append(GAMMA_HV) # APPEND THE HV LAYER\n",
    "    \n",
    "HV_MIN = np.min(np.array(DA_HV), axis=0) # ESTIMATE MIN OF THE HV LAYER ACROSS TIMESERIES\n",
    "HV_MAX = np.max(np.array(DA_HV), axis=0) # ESTIMATE MAX OF THE HV LAYER ACROSS TIMESERIES\n",
    "\n",
    "\n",
    "# CREATE HH MIN AND MAX LAYER \n",
    "DA_HH = []\n",
    "for NUM in range(NUM_SCENES):\n",
    "    # LOAD THE HV IMAGE\n",
    "    HH_INFO = gdal.Open(HH_FILES[NUM]) # LOAD THE HH IMAGE\n",
    "    GAMMA_HH = HH_INFO.GetRasterBand(1).ReadAsArray() # READ THE HH DATA INTO ARRAY  \n",
    "    DA_HH.append(GAMMA_HH) # APPEND THE HH LAYER\n",
    "    \n",
    "HH_MIN = np.min(np.array(DA_HH), axis=0) # ESTIMATE MIN OF THE HH LAYER ACROSS TIMESERIES\n",
    "HH_MAX = np.max(np.array(DA_HH), axis=0) # ESTIMATE MAX OF THE HH LAYER ACROSS TIMESERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b851a8-8f3a-4269-af35-4685b0706a88",
   "metadata": {},
   "source": [
    "##### Backscatter Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5507b-297e-4714-93cb-29bb3d74482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SAR MASK WITH VALID HH AND HV VALUES FROM MIN MAX LAYERS \n",
    "if DATABASE_NAME == 'DEMO':\n",
    "    THRESHOLD = 0.01\n",
    "else:\n",
    "    THRESHOLD = 0.001 # (Adjust threshold values according to site)\n",
    "\n",
    "# Create a backscatter mask\n",
    "SAR_MASK = np.where((HV_MIN > 0.01) & (HV_MAX < 1) & (HH_MAX < 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9c725-6531-420f-97b3-b54d50694594",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Get Land Cover Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f3186-d0ef-4345-a15c-f09e36f7b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_NAME == 'DEMO':\n",
    "    # Path to the lulc map\n",
    "    IN_LULC_FILE_LINK = 'https://s3.us-west-1.wasabisys.com/nisar-public-ebd/ATBD/ecosystems/disturbance/sites/leno/LENO_LULC.tif'\n",
    "    # Path to the directory where data will be streamed \n",
    "    IN_LULC_FILE = LULC_IN_DIR / Path(os.path.basename(IN_LULC_FILE_LINK))\n",
    "    # Stream and save directly to file\n",
    "    with requests.get(IN_LULC_FILE_LINK, stream=True) as r:\n",
    "        r.raise_for_status()  # raises error if status != 200\n",
    "        with open(IN_LULC_FILE, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "else:\n",
    "    # Path to the directory where data is stored \n",
    "    IN_LULC_FILE = '' # Path to the LULC file\n",
    "\n",
    "\n",
    "# OUTPUT PATH TO THE RESAMPLED LULC MAP\n",
    "LULC_NAME = 'NLCD' # name of the lulc used\n",
    "LULC_YYYY = 2021 # year of lulc used\n",
    "\n",
    "OUT_LULC_FILE = LULC_DIR  / Path(LULC_NAME + '_' + str(LULC_YYYY) + '_' + SITE + '_100m.tif')    \n",
    "\n",
    "# RESAMPLE THE LULC MAP FILE BY TAKING MODE OVER 100 m X100 m GRID OF REFERENCE IMAGE  \n",
    "fn.raster_clip(REFERENCE_FILE, IN_LULC_FILE, OUT_LULC_FILE, resampling_method = \"mode\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a11799-13d3-4781-ba17-0c6ef02e1bba",
   "metadata": {},
   "source": [
    "##### Plot resampled NLCD Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ea320-8f43-4bd6-afd1-7b1d71cc1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PLOT LULC IMAGE\n",
    "DATA_INFO = gdal.Open(OUT_LULC_FILE)\n",
    "# READ AGB VALUES TO ARRAY \n",
    "LULC =  DATA_INFO.GetRasterBand(1).ReadAsArray()\n",
    "# GET THE NLCD PALATTE \n",
    "HF = fn.get_color_palette_nlcd()\n",
    "# CREATE A CUSTOM COLORMAP\n",
    "CMAP = mpl.colors.ListedColormap(list(HF['Palette'].values))\n",
    "\n",
    "\n",
    "\n",
    "NORM = mpl.colors.BoundaryNorm(list(HF['Value'].values), ncolors=len(list(HF['Palette'].values)))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "# CREATES A FIGURE \n",
    "fig, ax = plt.subplots(figsize=(14, 6)) \n",
    "# DISPLAYS THE LULC IMAGE\n",
    "im = plt.imshow(\n",
    "    LULC,\n",
    "    cmap = CMAP,\n",
    "    norm = NORM, \n",
    "    extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    ")\n",
    "# ADD THE GRID TO IMAGE \n",
    "ax.grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "# ADD THE TITLE TO IMAGE \n",
    "ax.set_title(LULC_NAME + \" \" + str(LULC_YYYY) + \" Map\", fontweight = 'bold')\n",
    "# ADD THE XLABEL TO IMAGE \n",
    "ax.set_xlabel('Longitude (Deg)', fontweight = 'bold')\n",
    "# ADD THE YLABEL TO IMAGE \n",
    "ax.set_ylabel('Latitude (Deg)',  fontweight = 'bold')\n",
    "# FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# CREATE HANDLES FOR LEGEND\n",
    "handles = []\n",
    "for index, row in HF.iterrows():\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', label=row['Description'],\n",
    "                                markerfacecolor=row['Palette'], markersize=10))\n",
    "# DISPLAY THE LEGEND\n",
    "ax.legend(handles=handles, title=\"Land Cover Classes\", loc='lower right',\n",
    "          bbox_to_anchor=(1.3, 0), fontsize=8, borderaxespad=0.)\n",
    "# ADJUST PADDING AROUND THE IMAGE\n",
    "plt.tight_layout()\n",
    "# SAVE THE FIGURE \n",
    "plt.savefig(OUT_DIR  / Path(SITE + \"_LULC.png\"), dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9622b62-a054-41f5-a7a5-6a4f93589278",
   "metadata": {},
   "source": [
    "##### LULC Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b066c-6273-49ac-801d-01e1ee861634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A LULC MASK\n",
    "if DATABASE_NAME == 'DEMO':\n",
    "    LULC_MASK = (LULC != 11) & (LULC != 12) &  (LULC != 22) & (LULC != 23) & (LULC != 24)  \n",
    "else:\n",
    "    LULC_MASK = '' # (Remove the LULC classes which are non vegetated over the site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576ccca-002f-46d7-881a-5c7cb2040a14",
   "metadata": {},
   "source": [
    "##### Forest/Non-Forest (FNF) Scene Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77512c68-842d-42a5-85b1-5b560edfbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FNF MASK FOR NISAR SCENE\n",
    "SCENE_MASK = np.where((SAR_MASK == 1) & (LULC_MASK == 1), 1, 0)\n",
    "SCENE_MASK_FILE = OUT_DIR / Path(FILE_NAME + \"_scene_mask_100m.tif\") # CREATE A PATH TO SAVE THE MASK LAYER\n",
    "fn.write_geotiff_with_gdalcopy(REFERENCE_FILE, SCENE_MASK, SCENE_MASK_FILE) # SAVE THE MASK FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fada1-bc08-4c0c-a20a-99afba001a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot the Forest/Non-Forest (FNF) Scene Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d91e9-a162-4621-8cf4-250bc6ebdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "# CREATES A FIGURE \n",
    "fig, ax = plt.subplots(figsize=(16, 6)) \n",
    "# DISPLAYS THE AGB IMAGE\n",
    "im = plt.imshow(\n",
    "    SCENE_MASK,\n",
    "    cmap = \"gray\",\n",
    "    vmin = 0,\n",
    "    vmax = 1,\n",
    "    extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    ")\n",
    "# ADD THE GRID TO IMAGE \n",
    "ax.grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "# ADD THE TITLE TO IMAGE \n",
    "ax.set_title(\"FNF MASK\", fontweight = 'bold')\n",
    "# ADD THE XLABEL TO IMAGE \n",
    "ax.set_xlabel('Longitude (Deg)', fontweight = 'bold')\n",
    "# ADD THE YLABEL TO IMAGE \n",
    "ax.set_ylabel('Latitude (Deg)',  fontweight = 'bold')\n",
    "# FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# ADJUST PADDING AROUND THE IMAGE\n",
    "plt.tight_layout()\n",
    "# SAVE THE FIGURE \n",
    "plt.savefig(OUT_DIR / Path(SITE + \"_MASK.png\"), dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8739ef-1f38-4c48-b2e3-04a3002a90d8",
   "metadata": {},
   "source": [
    "##### AGB Mask (Cal/Val Only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf833c94-8c5b-4c88-b3f0-9e88bbec57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A AGB MASK\n",
    "if DATABASE_NAME == 'DEMO':\n",
    "    AGB_MASK = None  \n",
    "else:\n",
    "    AGB_MASK = (AGB > 0) & (AGB < 200) # (Adjust AGB values according to site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5d481-cd56-4871-9323-88121b73d563",
   "metadata": {},
   "source": [
    "##### Cal/Val Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b467b8e-568c-4c7c-af96-bbdb20447fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FNF MASK over Cal/Val Area\n",
    "if DATABASE_NAME == 'DEMO':   \n",
    "    print('Cal/Val Mask is require if you are calibrating the model')\n",
    "else:\n",
    "    MASK = np.where((AGB_MASK == 1) & (SAR_MASK == 1) & (LULC_MASK == 1), 1, 0)\n",
    "    MASK_FILE = OUT_DIR / Path(FILE_NAME + \"_mask_100m.tif\") # CREATE A PATH TO SAVE THE MASK LAYER\n",
    "    fn.write_geotiff_with_gdalcopy(REFERENCE_FILE, MASK, MASK_FILE) # SAVE THE MASK FILE\n",
    "\n",
    "    ##### Plot the FNF mask over Cal/Val Area\n",
    "    plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "    plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "    \n",
    "    # CREATES A FIGURE \n",
    "    fig, ax = plt.subplots(figsize=(16, 6)) \n",
    "    # DISPLAYS THE AGB IMAGE\n",
    "    im = plt.imshow(\n",
    "        MASK,\n",
    "        cmap = \"gray\",\n",
    "        vmin = 0,\n",
    "        vmax = 1,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "        )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax.grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax.set_title(\"FNF MASK\", fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax.set_xlabel('Longitude (Deg)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax.set_ylabel('Latitude (Deg)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADJUST PADDING AROUND THE IMAGE\n",
    "    plt.tight_layout()\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(OUT_DIR / Path(SITE + \"_MASK.png\"), dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7198240-9364-4403-aa2b-09ef7ef80391",
   "metadata": {},
   "source": [
    "#### Generation of Calibration/Validation Area\n",
    "The Forest areas are split into Calibration/Validation Area by dividing the data based on latitude, longitude, and a percentage of data.  For example, we can allocate a certain percentage (50%) of data across latitude for calibration, while the remaining percentage is used for validation. For this demonstration, we are not generating cal/val mask. However, for the completness of the workflow the steps are outlined. Enable the raw cell if you are performing this steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678a3c8-7c7b-4ce7-a444-6310f33ccc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FNF MASK over Cal/Val Area\n",
    "if DATABASE_NAME == 'DEMO':   \n",
    "    print('Since we the model parameters are already provide you dont need a calibration area')\n",
    "else:\n",
    "    # SPLIT THE AREA INTO CALIBRATION AND VALIDATION AREA \n",
    "    fn.create_mask_areas(MASK_FILE, OUT_DIR, PERCENTAGE = 50, IS_LATITUDE = True)\n",
    "\n",
    "    # #### Data Cleaning\n",
    "    # The data cleaning is used to remove outliers i.e. typically want to identify and remove values that are significantly different from the majority of your data. Outliers can arise due to noise, sensor errors, or other factors that do not represent the true underlying signal.\n",
    "    # * Aim to find the best set of training data\n",
    "    # * Removal procedure uses mean backscatter over time\n",
    "    # * Threshold of noise can be set at absolute and relative AGB levels\n",
    "    # * Input:\n",
    "    #   * One mask map identifying valid pixels\n",
    "    #   * One AGB map used for calibration\n",
    "    #   * List of NISAR scenes\n",
    "    #   * Absolute and relative noise levels\n",
    "    # * Output:\n",
    "    #   * Scatterplots showing radar backscatter vs. AGB\n",
    "\n",
    "    # PERFORM DATA CLEANING OVER CALIBRATION AREA \n",
    "    TRAIN_FILE = OUT_DIR / Path(FILE_NAME + \"_mask_100m_train.tif\" )\n",
    "    app = mr.scatter_plot_radar_agb(OUT_DIR, TRAIN_FILE, OUT_AGB_FILE, SAR_RES_DATA_LIST, FILE_NAME + '_train')\n",
    "    app\n",
    "\n",
    "\n",
    "    # PERFORM DATA CLEANING OVER VALIDATION AREA\n",
    "    TEST_FILE = OUT_DIR / Path(FILE_NAME + \"_mask_100m_test.tif\" )\n",
    "    app = mr.scatter_plot_radar_agb(OUT_DIR, TEST_FILE, OUT_AGB_FILE, SAR_RES_DATA_LIST, FILE_NAME + '_test')\n",
    "    app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f99ec-63c4-46d8-80c3-cee2e66fa648",
   "metadata": {},
   "source": [
    "### 3. NISAR Model Calibration & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf24dd6-127a-44ba-87fd-4c578e9f5358",
   "metadata": {},
   "source": [
    "Overview of Tasks Performed\n",
    ">**Initial Aboveground Biomass Estimation** : Initial estimates of AGB was estimated. Here HV polarizations values are used to estimate AGB values.<br/>\n",
    ">**Load the NISAR Biomass Model Parameters** : The initial estimate of NISAR biomass model parameters is a crucial step in developing accurate predictive model. In this study, initial estimates of the NISAR biomass model parameters are derived from the mean values of time series data gathered at the calibration site. These mean values, which capture the average trends and fluctuations of backscatter over time, provide a strong starting point for model calibration for time series estimation.<br/>\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150728bd-b087-4644-97bc-cd5399154dc0",
   "metadata": {},
   "source": [
    "#### Inital Parameter Estimation Over Calibration & Validation Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77cecb-1aee-4239-acf9-ddbf9122c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the initial AGB map\n",
    "mr.initial_agb_estimation(OUT_DIR, str(SCENE_MASK_FILE), SAR_RES_DATA_LIST, FILE_NAME, TRAIN_FILE = None, OUT_AGB_FILE = None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71911635-188b-4f7f-a939-d497b044895d",
   "metadata": {},
   "source": [
    "#### Load the calibration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5344f-d616-406f-a0c8-572e73fae9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "# Create configparser object\n",
    "CONFIG = configparser.ConfigParser()\n",
    "\n",
    "# Read the ini file\n",
    "CONFIG.read(MAIN_DIR / Path('config/config.ini'))\n",
    "\n",
    "\n",
    "PARAM  = CONFIG['NISAR'] \n",
    "PARAM0_FILE = OUT_DIR / Path(FILE_NAME + \"_model_sim_param0_s13.csv\")\n",
    "\n",
    "\n",
    "params = np.array([float(PARAM['A_HV']), float(PARAM['A_HH']), \n",
    "                    float(PARAM['B_HV']), float(PARAM['B_HH']),\n",
    "                    float(PARAM['C_HV']), float(PARAM['C_HH']),\n",
    "                    float(PARAM['alpha_HV']), float(PARAM['alpha_HH']),\n",
    "                    float(PARAM['delta_HV']), float(PARAM['delta_HH']),\n",
    "                    float(PARAM['D_HV']), float(PARAM['D_HH']),\n",
    "                    float(PARAM['S'])])\n",
    "\n",
    "\n",
    "param_names = [\n",
    "                  \"A_HV\",\n",
    "                  \"A_HH\",\n",
    "                  \"B_HV\",\n",
    "                  \"B_HH\",\n",
    "                  \"C_HV\",\n",
    "                  \"C_HH\",\n",
    "                  \"alpha_HV\",\n",
    "                  \"alpha_HH\",\n",
    "                  \"delta_HV\",\n",
    "                  \"delta_HH\",\n",
    "                  \"D_HV\",\n",
    "                  \"D_HH\",\n",
    "              ] + [\"S\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(params[:, None].T, columns=param_names)\n",
    "df.to_csv(PARAM0_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352fdd0-51cb-4926-a037-eeb897912c53",
   "metadata": {},
   "source": [
    "### 4. L3 Biomass Product Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0009e6-a17d-482e-a1e9-f67282398109",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.colormap_plot_agb_prediction(str(OUT_DIR), SAR_RES_DATA_LIST, SCENE_MASK_FILE,  str(PARAM0_FILE), FILE_NAME, ab_range=[54, 62])\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1f77f-198e-4571-98e6-e6c566241f28",
   "metadata": {},
   "source": [
    "#### Plot the Average AGB of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3a42-22c1-4b87-b5cc-83b5a49bdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "\n",
    "# LOAD THE PREDICTED AGB IMAGE\n",
    "PRED_AGB_FILE = OUT_DIR + '/output/' + FILE_NAME+'.tif'\n",
    "AGB_INFO = gdal.Open(PRED_AGB_FILE) # \n",
    "W0 =  AGB_INFO.GetRasterBand(1).ReadAsArray() \n",
    "# CREATES A FIGURE \n",
    "fig, ax = plt.subplots(figsize=(16, 6)) \n",
    "# DISPLAYS THE AGB IMAGE\n",
    "im = plt.imshow(\n",
    "    W0,\n",
    "    cmap = \"gist_earth_r\",\n",
    "    vmin = 0,\n",
    "    vmax = 100,\n",
    "    extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    ")\n",
    "# ADD THE GRID TO IMAGE \n",
    "ax.grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "# ADD THE TITLE TO IMAGE \n",
    "ax.set_title(\"Mean Predicted AGB\", fontweight = 'bold')\n",
    "# ADD THE XLABEL TO IMAGE \n",
    "ax.set_xlabel('Easting (m)', fontweight = 'bold')\n",
    "# ADD THE YLABEL TO IMAGE \n",
    "ax.set_ylabel('Northing (m)',  fontweight = 'bold')\n",
    "# FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "# ADJUST PADDING AROUND THE IMAGE\n",
    "plt.tight_layout()\n",
    "# SAVE THE FIGURE \n",
    "plt.savefig(OUT_DIR + SITE + \"_Predicted_mean_agb.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dfb80-7357-46e2-9290-40abaec98f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the AGB value of independent time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cb0f1-7a57-4fac-996a-f37af65bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Serif'  # SET DEFAULT FONT FAMILY FOR FIGURE \n",
    "plt.rcParams['font.serif'] = ['Times New Roman']  # SET THE PRIMARY FONT FOR ALL SERIF TEXT IN THE FIGURE  \n",
    "plt.rcParams['font.size'] = 10  # SET FONT SIZE \n",
    "\n",
    "fig, ax = plt.subplots(nrows=np.ceil(NUM_SCENES / 2).astype(int), ncols=2, \n",
    "                       figsize=(9, 4 * np.ceil(NUM_SCENES / 2)), sharex=True, sharey=True) \n",
    "\n",
    "ax1 = ax.flatten()\n",
    "for k in range(NUM_SCENES):\n",
    "    basename = os.path.basename(SAR_RES_DATA_LIST[k * 2])\n",
    "    PRED_AGB_FILE = OUT_DIR + '/output/' + FILE_NAME+ \"_agb_predictions_\" + basename[54:62] + \".tif\"\n",
    "    # LOAD THE PREDICTED AGB IMAGE\n",
    "    AGB_INFO = gdal.Open(PRED_AGB_FILE) # \n",
    "    W0 =  AGB_INFO.GetRasterBand(1).ReadAsArray() \n",
    "    # CREATES A FIGURE \n",
    "    im = ax1[k].imshow(\n",
    "        W0,\n",
    "        cmap = \"gist_earth_r\",\n",
    "        vmin = 0,\n",
    "        vmax = 100,\n",
    "        extent = [X_MIN, X_MAX, Y_MIN, Y_MAX],\n",
    "    )\n",
    "    # ADD THE GRID TO IMAGE \n",
    "    ax1[k].grid(True, which='both', axis='both', color='gray', linestyle=':', linewidth=0.5, zorder=1) \n",
    "    # ADD THE TITLE TO IMAGE \n",
    "    ax1[k].set_title(\"Predicted AGB \" + str(DATES[k]), fontweight = 'bold')\n",
    "    # ADD THE XLABEL TO IMAGE \n",
    "    ax1[k].set_xlabel('Easting (m)', fontweight = 'bold')\n",
    "    # ADD THE YLABEL TO IMAGE \n",
    "    ax1[k].set_ylabel('Northing (m)',  fontweight = 'bold')\n",
    "    # FORMAT THE XTICK VALUES TO TWO DECIMALS\n",
    "    ax1[k].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # FORMAT THE YTICK VALUES TO TWO DECIMALS\n",
    "    ax1[k].yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "    # ADJUST PADDING AROUND THE IMAGE\n",
    "    plt.tight_layout()\n",
    "    # SAVE THE FIGURE \n",
    "    plt.savefig(OUT_DIR + SITE + \"_Predicted_agb.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870558-bd96-4b17-94d8-07832da46d44",
   "metadata": {},
   "source": [
    "<a id=\"SEC_5\"></a>\n",
    "## [5. References](#SEC_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bccaac-4e06-439d-ad4b-032de2056074",
   "metadata": {},
   "source": [
    "1. Saatchi, S.S., McDonald, K.C., 1997. Coherent effects in microwave backscattering models for forest canopies. IEEE Transactions on Geoscience and Remote Sensing 35, 1032–1044. https://doi.org/10.1109/36.602545_\n",
    "\n",
    "2. Cushman, K.C.; Saatchi, S.; McRoberts, R.E.; Anderson-Teixeira, K.J.; Bourg, N.A.; Chapman, B.; McMahon, S.M.; Mulverhill, C. Small Field Plots Can Cause Substantial Uncertainty in Gridded Aboveground Biomass Products from Airborne Lidar Data. Remote Sens. 2023, 15, 3509. https://doi.org/10.3390/rs15143509\n",
    "\n",
    "\n",
    "[back to TOC](#TOC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
